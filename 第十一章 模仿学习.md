## 第十一章 模仿学习



#### Imitation Learning

##### Behavior Cloning

Behavioral Cloning 是一种纯监督学习的方法，那学习自动驾驶举例，在bahavior cloning的设置下，可以通过采集人类司机对于不同的状态的决策结果，形成数据集，并基于这个数据集进行训练，从而学到一个从observation到aciton的映射，也就是如果在某一个场景下人是向左打了方向，那么下次智能体看到这种情况后也会向左打方向。

![image-20210222111636836](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222111636836.png)

但是这么做会带来一个问题，也就是当我们遇到了不在数据集中存在的observation时，智能体就不知道怎么做了因此就可能会给出一个有偏差的action然后又进入了一个不在数据集中存在的observation，这样就会使得误差不断地进行累计，导致整个策略的效果变得很差。这个问题其实就是监督学习的泛化性的问题。

![image-20210222112150275](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222112150275.png)

##### Data Augmentation

针对上述的误差累积的问题，一个常见的方法时，Data Augmentation(数据增强）是一个常见的方法。如NVIDIA在自动驾驶的场景下，在车的三个方向装上摄像头，observation是摄像头的图像，action是方向盘的角度。对于正面的摄像头，期望的action就是和当前方向盘的方向一致。而对于左边和右边的摄像observation，对应的action则是将对应的方向盘方向分别向右和向左偏移对应的角度，从而就获得了相对原来三倍的数据，更重要的是，有了更多拐弯的数据，这些在正常的行驶中是占少数的，从而使得这个原先更可能出现误差累积的拐弯时刻变得更robust。

![image-20210222135437894](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222135437894.png)

##### DAgger

针对于上诉误差累积的问题，我们知道其实很重要的一个原因就是实际policy中observation空间和标注数据中observation空间相差比较大（这个问题也叫做Distributional drift，是监督学习中常见的一个问题），所以产生了很多我们在数据集中没见过的observation。而DAgger（Dataset Aggregation) 就是解决这个问题的一种方法：既然policy生成的observation不在label的observation之内，那么就将新生成的数据进行再次标注，并继续学习这些数据。下面也就是算法的流程，核心在第三步。

![image-20210222140312832](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222140312832.png)

但是在实际操作中，第三步其实是很困难的，因为“只给人类一张图片就让人做出决策”这件事情对于人来说是很困难的。

![image-20210222140515866](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222140515866.png)



#### Problem of Imitation Learning

在模仿学习中，误差主要来自两个方面，一个是对于未见过的observation的泛化误差，另一个是对于已见过observation的拟合误差。对于bahaviroal cloning和DAgger而言，都是在尽可能通过增加数据，减少第一种误差。

##### Non-Markovian bahavior

前面的博客里给大家分享过observation和state的差别，简单来说，state表示了当前状态（全部信息），而observation是state生成的结果（里面只有部分信息）。比如在自动驾驶这个场景下，人类在标注数据进行决策的时候，考虑到的并不仅仅是当前这一帧的observation，而是根据过去的许多信息来做出判断的，对于同样一帧图像，可能由于前面的信息不同，人类会做出不一样的决策。对于这样的数据，算法在学习的时候是有可能产生混淆的，这也就导致算法对于数据的拟合效果不够好。

![image-20210222143540359](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222143540359.png)

针对于这个问题，最直观的方法就是不只是把当前帧作为输入，而是使用过去若干帧作为输入并从中提取有用的信息，前面讲Nature DQN时也是这么做的。比如说我们可以用RNN或者LSTM去提取时序信息，这种做法带来的问题就是它需要更多的参数，从而使模型变得很大。

![image-20210222143855388](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222143855388.png)

在NIPS 2019上， Sergey Levine发表了一篇论文“ Causal Confusion in Imitation Learning”，指出在模仿学习中如果忽略因果性，可能会造成“ causal misidentification”，使得拿到越多的信息反而会造成越差的效果。这是因为信息越多的时候，代表着干扰信息（论文中称为confounder）也越多，因此在决策时，如果忽略了因果性，策略可能会觉得confounder时是造成结果的原因，而忽略了造成结果的真正的原因。比如在下面的两个图当中，左边的图比右边的图多了一个仪表盘的信息，每一次看到前方有行人时，专家策略都会踩下刹车，仪表盘的刹车灯就会亮，所以左边的智能体就认为应该在刹车灯亮的时候踩下刹车，这就与我们想要的完全不一致。而右边的图没有了仪表盘的信息反而不会出现这个问题。针对这个问题，像DAgger这种不断增加人工标注的方法就不能解决了，因为即使进行再多的标注，因果性的问题依旧会造成结构性的困难。

![image-20210222145729056](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222145729056.png)

##### Multimodal behavior

有时候在一个问题中可能存在多个可行解，比如在下图中有一颗树，专家可以往左边走，也可以往右边走。而当我们的输出值时连续值，使用高斯分布或者其他分布进行策略选择的时候，就可能导致我们最终的决策时直直地向前走。

![image-20210222150642041](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222150642041.png)

针对这个问题一般存在三种方法：

- Output mixture of Gaussians

  这个想法相对比较简单也比较直观，也就是我们在输出时不是输出一个高斯分布，而是对多峰进行模拟，这样就不会出现前面所说的那种“中和”的情况。但是，这种方法会随着问题维度的上升复杂度呈现指数级上升。

  ![image-20210222151202879](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222151202879.png)

- Latent variable models

  这种方法最终的输出还是一个高斯分布，只不过在输入上加入一些隐变量，这个隐变量我觉得也可以直接理解为噪声，噪声有用的原因在于对原数据产生了扰动，使得原先多峰的label变成了单峰的label，从而缓解多峰的问题，这种方法更加expressive，但是实现起来比较困难。在具体实现时，会有以下技巧：

  - Conditional variational autoencoder
  - Normalizing flow/realNVP 
  - Stein variational gradient descent

- Autoregressive discretization

  这种方式采用的是将动作空间离散化的方式，如果连续的action有很多个维度，则可以一个维度一个维度地输出。首先得到第一个维度的action，然后将它作为第二个维度的输出，并得到第二个维度的输出，如此循环。这种方法可以看成是前面两种方法的折中，既有比较强的表达能力，实现起来也不会太复杂。

![image-20210222152152893](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222152152893.png)



##### Limited Data

- 人类（专家）只能提供有限的数据，当数据足够丰富时，深度学习往往会表现更好。

- 人不擅长提供某一些类型的动作，比如下面的场景中：

  ![image-20210222152933077](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222152933077.png)

#### Theory Analysis of Imitation learning

首先，我们定义我们的cost function:

$$c(\mathbf{s}, \mathbf{a})=\left\{\begin{array}{l}
0 \text { if } \mathbf{a}=\pi^{\star}(\mathbf{s}) \\
1 \text { otherwise }
\end{array}\right.$$

也就是说当我们做的动作不是最优策略的动作时，我们会得到cost = 1

然后我们假设，我们的动作不是最优策略的动作的概率小于$$\epsilon$$,  然后我们举一个最极端的例子，就是让智能体去走钢丝，智能体需要一直向前走，一旦智能体没有想走前而是向左或者向右了，它就会掉入一个以前没有见过的状态。在这种情况下，让智能体去走T步，会得到：

$$E\left[\sum_{t} c\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \leq \underbrace{\epsilon T+(1-\epsilon)(\epsilon(T-1)+(1-\epsilon)(\ldots))}_{T}$$

也即这个期望最终会在$$O\left(\epsilon T^{2}\right)$$，表明单纯的Behavior Cloning的cost会随着随着决策步长的增加而呈现平方次增加。

当然我们可以让这个证明的泛化性变得更加强一点，也就是在实际中，我们其实不需要状态s都严格在train set中，我们只需要s在train set的分布中就行，因为泛化性的存在，也就是对$$\mathbf{s} \sim p_{\text {train }}(\mathbf{s})$$，$$E_{p_{\text {train }}(\mathbf{s})}\left[\pi_{\theta}\left(\mathbf{a} \neq \pi^{\star}(\mathbf{s}) \mid \mathbf{s}\right)\right] \leq \epsilon$$，在这种情况下，

$$\left.p_{\theta}\left(\mathbf{s}_{t}\right)=(1-\epsilon)^{t} p_{\operatorname{train}}\left(\mathbf{s}_{t}\right)+\left(1-(1-\epsilon)^{t}\right)\right) p_{\text {mistake }}\left(\mathbf{s}_{t}\right)$$

前一项表明我们不犯错的概率，后一项表明其他的分布。

$$\left|p_{\theta}\left(\mathbf{s}_{t}\right)-p_{\text {train }}\left(\mathbf{s}_{t}\right)\right|=\left(1-(1-\epsilon)^{t}\right)\left|p_{\text {mistake }}\left(\mathbf{s}_{t}\right)-p_{\text {train }}\left(\mathbf{s}_{t}\right)\right| \leq 2\left(1-(1-\epsilon)^{t}\right)$$

$$\text { useful identity: }(1-\epsilon)^{t} \geq 1-\epsilon t \text { for } \epsilon \in[0,1]$$

上面那一项就会小于$$2 \epsilon t$$

$$\begin{aligned}
\sum_{t} E_{p_{\theta}\left(\mathbf{s}_{t}\right)}\left[c_{t}\right]=\sum_{t} \sum_{\mathbf{s}_{t}} p_{\theta}\left(\mathbf{s}_{t}\right) c_{t}\left(\mathbf{s}_{t}\right) \leq \sum_{t} \sum_{\mathbf{s}_{t}} p_{\text {train }}\left(\mathbf{s}_{t}\right) c_{t}\left(\mathbf{s}_{t}\right)+\left|p_{\theta}\left(\mathbf{s}_{t}\right)-p_{\text {train }}\left(\mathbf{s}_{t}\right)\right| c_{\max } \\
\leq \sum_{t} \epsilon+2 \epsilon t \\
O\left(\epsilon T^{2}\right)
\end{aligned}$$

和上面的结果一样。

#### Another way to do imitation learning

##### Goal-conditional

如下图所示，如果我们针对一个任务p1采集到了很多的轨迹，那么我们就可以利用它做Imitation learning，但是如果我们针对不同的任务，每个任务都采集到了一条轨迹，那么对于其中的任意一个任务来说去做imitation learning的数据量都是不够的，但是我们可以把这些数据利用起来去处理goal-conditional的任务，也就是学习针对不同任务的模仿学习，我觉得这个思想其实是HER和Imitation learning的结合。

![image-20210222164725194](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222164725194.png)

这里给大家简要介绍相关的两篇论文：

一篇是讲Goal-conditional behavioral cloning的，名字叫做**Learning Latent plans from Play**，

针对一段Play交互的序列:
$D=\left\{\left(s_{1}, a_{1}\right),\left(s_{2}, a_{2}\right),\left(s_{3}, a_{3}\right),\left(s_{4}, a_{4}\right),\left(s_{5}, a_{5}\right)\left(s_{6}, a_{6}\right)\right\}$
该交互序列可能包含多个任务的执行, 如 $s_{1} \sim s_{3}$ 执行开门任务, $s_{4} \sim s_{6}$ 执行拉抽匹的任 务。如果直接拟合 $s_{n} \rightarrow a_{n}$ 的关系, 将无法编码这种与任务相关的知识。因此, 本文学习一个 goal-conditional 策略[Hinsight experience replay. NIPS-17]. Goal可以理解为未来的状态, 例如, 将 $s_{3}$ 作为 goal, 则智能体可以知道当前进行的开门的任务, 应该输出与开门相关的动作; 将 $s_{6}$ 作为 goal, 则智能体应该输出与拉抽越相关的动作。通过将目标作为策略的一部分, 可以建模 Goal-conditional 策略 $\pi\left(a_{t} \mid s_{t}, g\right)$ 。

就比如在下面的环境中，机械臂可以不只进行一个任务：拉抽屉、抓取物体、开门、按按钮等。希望从Play数据中学到任务无关的skill. 这与模仿学习（imitation）不同，模仿学习一般针对的是单任务。

![image-20210222165514829](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222165514829.png)

另一篇叫做**Goal-conditioned imitation learning**。

这篇论文**Goal-conditioned imitation learning**将HER的思想运用到了imitation learning的setting下，定制出了一种reward，使得我们的agent可以达到任意我们想去的goal state，并且最终agent**可以比专家更厉害**，比起传统的imitation learning，这篇论文最大的区别就是goal的引入，以及用goal结合HER定制出的新reward function。

##### Going beyond just imitation

这个想法的关键在于我们通过刚开始做随机策略来进行模仿？主要就是下面的这篇论文**Learning to Reach Goals via  Iterated Supervised Learning**.

这篇论文的主要贡献在于：

- 提出使用 hindsight 的思想，可以将智能体交互产生的 non-optimal 轨迹进行 Goal Relabel。在替换 Goal 之后，该样本在新的 Goal 下是 optimal 的样本，可以被当做专家轨迹进行学习。
- 在理论上证明了这种方法能够在理论上达到 optimal.

其主要步骤为：

- 首先, 给定一个采样的目标, 策略 condition 在该目标上执行交互产生一条轨迹。但是由于策略并不是最优的轨迹。
- 根据这条非最优的轨迹，我们可以通过替换Goal来使得该轨迹变为最优。
- 根据上述方法, 对一条长度为 $T$ 的非最优轨迹, 可以采样出 $C_{T}^{2}$ 条最优的轨迹。
- 在该集合上，训练策略来最大化似然, 其中 $s=s_{t}, g=s_{t+h}$ :

![image-20210222170233295](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210222170233295.png)

### 