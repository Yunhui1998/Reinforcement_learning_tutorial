## 第九章-单调提升的策略优化

#### Problems in Policy Gradient

到这里为止, policy gradient中一直存在两个比较重要的问题。

首先是sample efficiency的问题：由于算法是on-policy的，每次进行完梯度上升后都将数据丢 弃。而在lecture5中引入了importance sampling进行off-policy的policy gradient, 这样存在的 问题则是稳定性降低了，推导式中存在两个 policy连乘的比值, 比较容易出现消失或者爆炸的现象:
$$
\begin{aligned}
\nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right) &=E_{\tau \sim \pi_{\theta}(\tau)}\left[\frac{\pi_{\theta^{\prime}}(\tau)}{\pi_{\theta}(\tau)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\tau) r(\tau)\right] \quad \text { when } \theta \neq \theta^{\prime} \\
&=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\prod_{t=1}^{T} \frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\right)\left(\sum_{t=1}^{T} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right]
\end{aligned}
$$
在后续介绍如何从更加理论层面讨论两个policy之前差异带来的影响。
第二个则是来自gradient ascent的, 由于它是在parameter space上做的更新, 但是其实 parameter space并不等价于policy space, 可以看到如下的一个例子:
Consider a family of policies with parametrization:
$$
\pi_{\theta}(a)=\left\{\begin{array}{ll}
\sigma(\theta) & a=1 \\
1-\sigma(\theta) & a=2
\end{array}\right.
$$
![preview](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/v2-07f089c50ed3899bcf0fe6dadefbc7f4_r.jpg)

参数空间上一个小的变化在策略（动作空间）上带来了巨大的差异。

上面两个问题, 也就给算法提出了更多的要求：

如何在引入importance sampling的情况下尽可能避免policy差异过大的问题。 如何在保证policy不发生突变的情况下进行参数的更新。

#### Issues of Importance Sampling

我们在off-policy的方法中经常会用到importance sampling的方法，那么我们用一个分布去对另一个分布的期望进行求解的这种方法有什么问题呢？---问题就在分布的方差上面：

我们知道：

​                                                                                                  $$\begin{array}{l}\operatorname{VAR}[X] =E\left[X^{2}\right]-(E[X])^{2}\end{array}$$

而在重要性采样中，期望和原来的期望没有差别：

$$E_{x \sim p}[f(x)]=E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]$$

那我们来看一下importance sampling的方差有什么差异：

​                                                                                                 $$\begin{array}{l}
\operatorname{Var}_{x \sim p}[f(x)]=E_{x \sim p}\left[f(x)^{2}\right]-\left(E_{x \sim p}[f(x)]\right)^{2} \\
\begin{aligned}
\operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right] &=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2} \\
&=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)}-\left(E_{x \sim p}[f(x)]\right)^{2}\right.
\end{aligned}
\end{array}$$



我们发现唯一的差别就在$$\frac{p(x)}{q(x)}$$，也就是说当$$\frac{p(x)}{q(x)}$$值很大或者很小的时候，方差就会差的比较多。

我们可以看下面的图：

当p(x)和q(x)差的比较多的时候，如果我们采样在左边，那么最终得到的期望就是负的，如果采样在右边，那么最终得到的期望就是负的。当然如果采样的数量足够多，即使我们采样的大部分的点都在右边，但是之后采到了左边一些点，但是因为$$\frac{p(x)}{q(x)}$$比值比较大，那么这个权重就会把左边的这个负值放大，所以其实如果采样的数量足够多，这个方差的问题还是不会存在，但是事实是我们现实中采样的数量都是有限的。

![image-20210308113041201](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308113041201.png)

#### Monotonic Improvement Theory（策略的单调提升）

在这一部分，我们希望能找到一种方法，使得策略模型在优化的过程中单调提升，要做到这一点，我们首先要解决的一个问题就是怎么衡量两个策略（更新前和更新后的策略）之间的差异：

​                                                                                          $$\begin{aligned}
J\left(\theta^{\prime}\right)-J(\theta) &=J\left(\theta^{\prime}\right)-E_{\mathbf{s}_{0} \sim p\left(\mathbf{s}_{0}\right)}\left[V^{\pi_{\theta}}\left(\mathbf{s}_{0}\right)\right] \\
&=J\left(\theta^{\prime}\right)-E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[V^{\pi_{\theta}}\left(\mathbf{s}_{0}\right)\right] \\
&=J\left(\theta^{\prime}\right)-E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)-\sum_{t=1}^{\infty} \gamma^{t} V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right] \\
&=J\left(\theta^{\prime}\right)+E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right] \\
&=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]+E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right] \\
&=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right] \\
&=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
\end{aligned}$$

在这里我们稍微介绍一些上面的证明做了：

- 第一行：将old policy展开为对它生成的trajectory的期望。
- 第二行：由于两个策略关于价值函数的expectation是相同的，所以可以将对old policy的期望转化为new policy的期望，这是这里转化的最关键一步。
- 在三到四步，做了一个简单的构造，
- 在第五步将new policy拆分，由于两个部分都转化为关于new policy的expectation，所以就可以将它们合并，则可以得到最终的关系式。

也就是说，我们的新策略与旧策略的差就是旧策略的advantage关于新策略的trajectory的expectation。这个时候，如果按照policy iteration的流程，在improvement中，也就只需要使得每步提升最大，找到新的parameter使得下面的这个式子最大化即可。

​                                                                                     $$J\left(\theta^{\prime}\right)-J(\theta)=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$$

然后我们对后面一项进行重要性采样：

​                                                                                   $$\begin{aligned}
E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] &=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right] \\
&=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]
\end{aligned}$$



我们发现前面一项还是存在新策略的expectation的问题，使得这个问题就变得异常复杂，那么我们能不能在这里直接把新策略替换成旧策略呢？这样我们就可以根据旧策略去算这个值了：

![image-20210308115646170](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308115646170.png)

这里我们有一个假设或者可以说是一个先验，也就是当我们的新策略与旧策略比较接近时，我们得到的$$p_{\theta}\left(\mathbf{s}_{t}\right)$$和$$p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)$$就会比较接近，那么我们就可以对前面两项期望做替换。

那么我们现在就来证明它：

首先证明确定性策略的情况下：

![image-20210308132638838](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308132638838.png)

接着证明策略是任意分布下的情况：

![image-20210308132721992](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308132721992.png)

最终也就得到了一个bound，只要两个policy在policy space足够相近，那么就可以直接对expectation进行替换，进行policy improvement，同时能够有收敛保证：
$$
\theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]
$$
such that $\left|\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right| \leq \epsilon$
for small enough $\epsilon,$ this is guaranteed to improve $J\left(\theta^{\prime}\right)-J(\theta)$



#### Monotonic Improvement with KL Divergence

上面直接关于policy probability做差取绝对值的约束实际上是比较难优化的, 所以我们希望能够找个一个更容易优化的约束函数, 从而降低求解难度。所以在这里我们引进KL散度：

关于两个policy distribution的KL divergence可以由如下式子定义:
$$
D_{\mathrm{KL}}\left(p_{1}(x) \| p_{2}(x)\right)=E_{x \sim p_{1}(x)}\left[\log \frac{p_{1}(x)}{p_{2}(x)}\right]
$$
KL散度表征的是策略之间的差异。

从它的性质也可以推出如下的性质:
$$
\left|\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right| \leq \sqrt{\frac{1}{2} D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) \| \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)}
$$
也就是说如果我们将KL divergence约束住了，那么原始约束函数也就满足了，两者存在转化的等价性。所以优化的目标也就转化为如下的形式, 约束发生了改变, 问题更容易优化了。

![image-20210308161628830](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308161628830.png)

在上面将整个问题转化为对KL divergence约束的优化问题，那么在这里就将讨论各种求解这个优化问题的方式。根据不同的近似方法，就可以得到不同的算法。

![image-20210308161957370](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308161957370.png)

#### 共轭梯度法实现策略上的单调提升(Monotonic Improvement with  Dual gradient descent)

我们先介绍一下共轭梯度法，再介绍一下共轭梯度法在策略的单调提升上的应用。

##### 共轭梯度法的目的：让每一次优化变得“极致”

前面我们一般都用策略梯度法去解决优化问题。虽然梯度下降法的每一步都朝着局部最优的方向前进，但它在不同的迭代轮数会选择非常相近的方向，这说明当某一次选择了一个更新方向和步长后，这个方向并没有被更新完，未来还会存在这个方向的残差。如果把参数更新的轨迹显示出来，我们可以看到有时轨迹会走成 下面这种zig-zag 的形状：

![image-20210309110544217](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210309110544217.png)

于是，我们对优化提出了更高的要求：**每朝一个方向走，就把这个方向优化到极致，使得之后的更新再也不需要朝这个方向进行**。于是我们引出了一个变量一一误差，假设最优的参数为$x^{*},$ 当前第 $t$ 轮的参数为 $x_{t},$ 误差可以定义为
$$
e_{t}=x^{*}-x_{t}
$$
这个误差表示了参数的最优点和当前点之间的距离。那么目标就更明确了，我们**希望每一步优化后，当前的误差和刚才优化的方向正交**。注意，这里的优化方向不一定是梯度。现在令 $r_{t}$ 表示第 $t$ 轮的更新方向，就可以得到下面的公式：

​                                                                                                                              $r_{t}^{\mathrm{T}} e_{t+1}=0$
**假设每一轮的优化量都和误差正交，那么如果我们的优化空间有 $d$ 维，理论上最多只需要迭代 $d$ 轮就可以求解出来，这样在优化时间上就有了保证**。如果我们直接使用这个公式，就会发现一个问题: 公式中需要知道误差。换句话说，我们需要知道最优点在哪儿。如果我们知道最优点在哪儿，就不用优化了。 可是不知道最优点儿，这个方法又无法直接使用，仿佛陷入了一个死循环，那么接下来我们就利用数学工具来解决这个问题。

##### 共轭梯度法的推导

前面我们提到优化方向和误差正交，如果使用了共切这个工具，现在两者的关系将变为共轭正交，也就是存在一个矩阵 $A(A$就是轭 )，使得优化方向和误差这两头牛“牵手成功”，满足正交的性质:
$$
r_{t}^{\mathrm{T}} A e_{t+1}=0
$$
等等，这不就说明轭的存在使原本正交的两项变得不正交了吗? 但其实如果我们在上面的原始公式中间加一个特殊的矩阵，例如单位阵：
$$
r_{t}^{\mathrm{T}} \boldsymbol{I} \boldsymbol{e}_{t+1}=0
$$
加入单位阵并不会改变结果，所以原本正交的二者依然正交。所以换个角度理解，**我们以前简单的正交都是在单位阵这个简单且性质优良的“轭”的下实现的**。所以回到正交中，如果使用单位阵作为轭, 那么绑定在一起的就是常规意义上正交的一对向量; 如果使用其他矩阵，那么共轭正交的向量肯定也会满足其他的性质。

明确了共轭梯度法的目标和特点，我们就要开始推导算法公式了。共轭梯度法属于线搜索的一种，因此和梯度下降法类似，我们的总体思路不变，优化过程分如下两步:

- 确定优化方向

- 确定优化步长。

我们先来介绍优化步长的计算方法。假设当前的参数为 $X_{t},$ 我们已经得到了优化方向 $r_{t},$ 下面要确定的就是步长 $\alpha_{t},$ 根据前面提过的共轭正交公式
$$
\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{e}_{t+1}=0
$$
我们可以开始推导:
$$
\begin{aligned}
r_{t}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{e}_{t+1} &=\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A}\left[\boldsymbol{e}_{t}+\boldsymbol{X}_{t}-\boldsymbol{X}_{t+1}\right] \\
&=\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A}\left[\boldsymbol{e}_{t}+\alpha_{t} \boldsymbol{r}_{t}\right] \\
&=\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{e}_{t}+\alpha_{t} \boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{r}_{t}=0
\end{aligned}
$$
于是可以整理得到
$$
\begin{array}{l}
\alpha_{t}=-\frac{r_{t}^{\mathrm{T}} A e_{t}}{r_{t}^{\mathrm{T}} A r_{t}} \\
\alpha_{t}=-\frac{r_{t}^{\mathrm{T}} A\left(\boldsymbol{X}^{*}-\boldsymbol{X}_{t}\right)}{\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A} r_{t}}
\end{array}
$$
我们知道 $A X^{*}=b,$ 第 $t$ 轮的梯度 $g_{t}=A X_{t}-b,$ 于是公式最终变为
$$
\alpha_{t}=\frac{\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{g}_{t}}{\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A} r_{t}}
$$
**到这里，我们利用矩阵 $A$ 成功地把公式中的误差 $e$ 抵消，于是步长变得可解。**
完成了步长的求解，接下来就要回到第一步看看优化方向的计算方法。我们要解决的主要问题是如何让优化方向和误差正交。由于每一次的优化后，剩下的误差和本次的优化正交（共轭正交 ) , 所以可以看出每一个优化方向彼此间都是正交的。那么，我们接下来就用Gram-Schmidt 方法来构建这些彼此正交的向量。

在线性代数课上, 我们曾经学过一个向量正交化的方法一 Gram-Schmidt 方法。这
个算法的输入是 $N$ 维空间中 $N$ 个线性无关的向量，由于向量间线性无关，任何一个向
量都无法通过其他向量表达出来。算法的输出是 $N$ 个相互正交的向量，也就是我们最
终想要的向量组合。
它的具体算法如下，令输入向量为 $u_{1}, u_{2}, \cdots, u_{N},$ 输出向量为 $d_{1}, d_{2}, \cdots, d_{N},$ 那 么有:
( 1 ) 对于第一个向量，我们保持它不变: $u_{1}=d_{1} \circ$
(2) 对于第二个向量，我们去掉其中和第一个向量共线的部分，令去掉的比例为 $\beta_{i},$ 所以第二个向量 $d_{2}$ 等于 $u_{2}+\beta_{1} d_{1 \circ}$
( 3 ) 对于第三个向量，我们去掉其中和第一、第二个向量共线的部分: $d_{3}=\boldsymbol{u}_{3}+$ $\sum_{i=1}^{2} \beta_{3, i} d_{i \circ}$
(4) 对于第 $N$ 个向量，我们去掉其中和第一、第二、第 $N-1$ 个向量共线的部分 :
$\boldsymbol{d}_{N}=\boldsymbol{u}_{N}+\sum_{i=1}^{N-1} \beta_{N, i} \boldsymbol{d}_{i \circ}$
那么我们怎么求这些比例项 $\beta$ 呢? 我们利用前面提到的性质，向量之间正交（这
里还是共轩正交 )，于是有
$$
d_{l}^{\mathrm{T}} A d_{t}=0,(l=1,2, \ldots, t-1)
$$
进一步展开，可以得到
$$
\boldsymbol{d}_{l}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{d}_{t}=\boldsymbol{d}_{l}^{\mathrm{T}} \boldsymbol{A}\left(\boldsymbol{u}_{t}+\sum_{i=1}^{t-1} \beta_{t, i} \boldsymbol{d}_{i}\right)=\boldsymbol{d}_{l}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{u}_{t}+\boldsymbol{d}_{l}^{\mathrm{T}} \boldsymbol{A} \sum_{i=1}^{t-1} \beta_{t, i} \boldsymbol{d}_{i}
$$


















#### 自然梯度法实现策略上的单调提升(Monotonic Improvement  with Natural gradient descent)

#### TRPO实现策略上的单调提升(Monotonic Improvement  with TRPO )

#### PPO实现策略上的单调提升(Monotonic Improvement  with PPO )

#### GAE

### Off-policy Policy gradient

#### Retrace

#### ACER

#### DPG

#### DDPG















