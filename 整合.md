### 第一章 强化学习的基本介绍

#### 强化概念的来源

在动物学习背景下，“强化”这个术语在Thorndike表达效力定律后得到了很好的应用。 在1927年巴甫洛夫关于条件反射的专著的英文译本中，首先出现在这种背景下： **巴甫洛夫将强化描述为由于动物接受刺激 - 一种强化剂 - 与另一种刺激或反应有适当的时间关系而加强行为模式**。 一些心理学家将强化的观点扩展到包括削弱和加强行为，并扩展强化者的想法，包括可能**忽略或终止**刺激。 要被认为是增强剂，强化或弱化必须在强化剂被撤回后持续存在；仅仅吸引动物注意力或刺激其行为而不产生持久变化的刺激物不会被视为强化物。

#### 强化学习的基本概念

![image-20200827105730668](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200827105730668.png)



**强化学习是指与复杂、不确定的环境进行交互时，最大化从环境获得的累计奖励的一种方法。**

####  强化学习和监督学习的差别

- Sequential data as input (not i.i.d)   

  （输入的是时序的数据而不是像监督学习那样输入的数据满足独立同分布的要求）

- The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. 

  （没有一个“监督者”，只有通过奖励信号来判断应该采取哪个动作）

- Trial-and-error exploration (balance between exploration and exploitation）

  （探索与利用之间的平衡）

- The data used to train the agent is collected through interactions with the environment by the agent itself (compared to supervised learning where you have a fixed dataset for instance). This dependence can lead to vicious circle: if the agent collects poor quality data (e.g., trajectories with no rewards), then it will not improve and continue to amass bad trajectories.

  （训练智能体的数据是智能体自身和环境交互得到的，而不像监督学习那样一开始有一个固定的数据集，**这样就可能带来一个恶性循环**：如果智能体收集到一些很差的数据，比如一些奖励为0的轨迹，那么智能体通过这些数据可能得不到提升，进一步地智能体又采集到一些质量很差的数据，陷入恶性循环）。

- **具有超人类的上限**：传统的机器学习算法依赖人工标注好的数据，从中训练好的模型的性能上限是产生数据的模型（人类）的上限；而强化学习可以从零开始和环境进行不断地交互，可以不受人类先验知识的桎梏，从而能够在一些任务中获得超越人类的表现，比如AlphaGo、AlphaZero的表现就超越了人类。

- **延迟性**：得到的结果很可能延迟于我们做出决策一段时间，有些糟糕的影响并不完全因为当前的决策错误导致的，可能由于前面某一个步骤造成了一个不可逆的错误。

- **非监督的**：我们通常只得到reward signal。每次系统的action只能得到代表这次行为的好坏的标量，比如是10 points，但是我们不知道他的最好的值是多少，就可以理解为一个老师给你打了10分，你其实不知道这是百分制的10分还是十分制的10分。

#### 进化策略与强化学习

进化策略(Evolution Strategies, ES)反复迭代调整一个正态分布进行搜索来优化算法，是一种无梯度随机优化算法。进化策略中迭代的正态分布一般写成 $N\left(m_{t}, \sigma_{t}^{2} C_{t}\right),$ 包 含三个参数 $m_{t}, \sigma_{t}, C_{t}$ 。正态分布的参数所起的作用为：

- $m_{t}$ 均值，决定分布的中心位置; 在算法中，决定搜索区域;
- $\sigma_{t}$ 步长参数, 决定分布的整体方差(global variance); 在算法中，决定搜索范围的大小和强度。
- $C_{t}$ 协方差矩阵，决定分布的形状; 在算法中决定变量之间的依赖关系，以及搜索方向之间的相 对尺度(scale).

ES算法设计的核心就是如何对这些参数进行调整, 尤其是步长参数和协方差矩阵的调整, 以达到尽可能好的搜索效果。对这些参数的调整在ES算法的收敘速率方面有非常重要的影响。一般的, ES调整参数的基本思路是, 调整参数使得产生好解的概率逐渐增大（沿好的搜索方向进行搜索的 概率增大）。
进化策略在搜索中反复迭代以下步骤：

1. Sampling: 采样产生一个或者一组候选解(candidate solutions);

2. Evaluation：对新产生的解计算对应的目标函数值;

3. Selection：依据目标函数值选择部分或者全部解;

4. Update：使用选择的解更新分布参数.
   在进化算法中，一次完整的迭代称为一代 (generation)，一个候选解称为一个个体，计算目标计算目标函数值的过程称为评估。每次迭代产生的新的候选解称为子代（offspring），通过选择得到的用于产生子代的解称为父代（parent）。

  如下图所示：

![preview](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/v2-aaf5c6d5cb52dc04d8d43f468ac53bee_r.jpg)

进化采取大量静态策略，每个策略在扩展过的较长时间内与环境的一个独立实例进行交互，然后选择最多收益的策略及其变种来产生下一代的策略，然后继续循环更替，**它在单个个体的生命周期内不进行学习，它没有利用强化学习中这种交互的特性，其实更像是一种策略搜索**。

为了评估策略，进化方法保持策略固定并且针对对手进行多场游戏，或者使用对手的模型模拟多场游戏。 胜利的频率给出了对该策略获胜的概率的无偏估计，并且可用于指导下一个策略选择。 **但是每次策略进化都需要多场游戏来计算概率，而且计算概率只关心最终结果，*每场游戏内* 的信息被忽略掉了**。 例如，如果玩家获胜，那么游戏中的 *所有* 行为都会被认为是正确的，而不管具体移动可能对获胜至关重要。 甚至从未发生过的动作也会被认为是正确的！ 相反，值函数方法允许评估各个状态。 

最后，进化和价值函数方法都在搜索策略空间，但价值函数学习会利用游戏过程中可用的信息。

#### 强化学习中的基本概念

可以参考知乎上[这篇文章](https://zhuanlan.zhihu.com/p/30315707)的内容:

- Policy :A policy is the agent's behavior model . It is a map function from state/observation to action. Policy（策略）其实就是强化学习中最终要得到的模型，**策略是一个状态到动作的映射**，也就是说告诉Policy当前状态是什么样的，它会返回给你一个动作。

  - Stochastic policy: Probabilistic sample: $\pi(a \mid s)=P\left[A_{t}=a \mid S_{t}=s\right]$
  - Deterministic policy:$a^{*}=\arg \max _{a} \pi(a \mid s)$

- Agent：智能体，负责环境进行交互。

- Reward:   reward是一个**量化的反馈信号**。它决定了对于智能体来说什么是好，什么是坏（短时间），**强化学习的最终目标其实就是最大化累积奖励**。而相比于reward，我们后面要学的**value function其实一个更长远角度的一个过程**，**但是估计价值其实是一个很难的过程**。

  举个例子来说，如果我们强化学习的应用场景是开直升飞机，那么如果直升飞机按照我们期望的轨迹飞行，我们就会给智能体一个正的奖励，如果它坠机了，我们就会给它一个负的奖励。当然，在实际问题中，具体奖励给多少，怎么给，其实是一个很复杂的问题。

- Action：智能体的动作。

- Goal：我们要实现的目标，对于强化学习任务本身来说，就是最大化累计奖励，所以在强化学习中很关键的一点就是保证最终要达到的目标和最大化累计奖励的目标是一致的: 智能体可能会学出一个策略去最大化累积奖励，但是根本不是我们想要的方式。

- Observation： 智能体观察到的东西，比如说直升机当前的位置和速度。

- Environment：环境。有的书里面会花很多篇幅去区分智能体和环境，去纠结哪个是属于智能体，哪个是属于环境，我觉得对于像我这样的初学者来说，不用过度纠结，你就把智能体想象成自己，把除你自己以外的所有东西都想象成环境。

- Trajectory：轨迹，可以理解成智能体从开始到结束是怎么一步一步过来的。

- Rollout:  rollout在字典中的意思是：首次展示，滑跑。在强化学习中大家就可以理解成一次实验、一条轨迹。

- Value function （expected discounted sum of future rewards under a particular policy $\pi$, **we use it to quantify goodness/badness of states**（这里的有多好是用未来预期的收益来定义的），可以简单地理解为**价值函数就是来判断状态的好坏**。

  ​                                                                                            $v_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s\right],$ for all $s \in \mathcal{S}$

- Model: A model predicts what the environment will do next。注意这里的model不是我们最终学出来的做决策的模型，而是当前状态S和动作A到下一个动作S‘的映射的模型，也就是告诉你现在如果你这么干你会得到什么后果的模型。

  - Predict the next state:$\mathcal{P}_{s s^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right]$
  - Predict the next reward:$\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]$
  - model-based 和model-free的差别就是知不知道状态转移矩阵和reward

- Q-function **(could be used to select among actions)**:$q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s, A_{t}=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s, A_{t}=a\right]$

  **Q函数其实是对（S，A）对的一个评价，也就是说从长期的角度来看（reward是短期），你在当前状态做某一个动作到底有多好**。所以如果我们知道了Q函数，其实就知道了在每一步到底应该怎么做了，最简单的就是取agmax选Q函数最大的动作，根据贝尔曼方程，我们如果知道V函数，我们可以求出Q函数，所以大家会看到说，我们如果知道了Q函数或者V函数，这个强化学习问题就是可解的。

- history是一个action、observation、reward的序列：

  ​                                                                                              $$H_{t}=A_{1}, O_{1}, R_{1}, \ldots, A_{t}, O_{t}, R_{t}$$

  history能够决定action接下来采取什么行动；实际上，agent做的就是从history到action的映射；对于环境而言，输入history和智能体产生的action，输出对应的observation和reward。

- State：是history的一个总结，用来决定下一步的action是什么，实际上，他是一个关于history的函数，我们可以这样表示：

​                                                                                                        $$S_{t}=f\left(H_{t}\right), A_{t}=h\left(S_{t}\right)$$

​         State相对于history而言最大的好处在于他的信息量很少，实际上，我们也不需要得到所有的history来得到下一步的action。

​         在这里想讲一下state和observation的区别，也就是大家有时候会看到$\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{o}_{t}\right)$，有时候看到$\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)$，这两个其实是存在区别的，用Sergey Levine来说就是，**States are the true configuration of the system and an observation is something that results from that state which may or may not be enough to deduce the state.**  **State 反映了系统的真实的信息，observation是state表现出来的结果，并不能完全反映出state中的信息。**举个例子，有一只老虎在我们的面前，老虎在哪，速度多少这些就算是state，而我们观察的时候，这只老虎刚好被一棵树挡住了，也就是说我们的observation是一张被树挡住的老虎的照片，那么我们仅仅根据observation（照片）是不能决定我们现在要不要赶紧跑的。而这其实也说明了**当state满足马尔科夫性的时候，observation是不一定满足的**，也就是我们不能根据当前的这张照片来判断我们是不是该不该跑，但是结合之前的一些照片比如老虎一步步潜伏到树底下的照片，我们就可以知道，该跑了！

​           用下面这张图可以帮助比较好地理解Observation和State之间的关系：

![image-20210110114937403](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210110114937403.png)

- Bootstrapping: 指用上一次估计值来更新本次的估计值。 比如用上一次估计的表格中的下一状态s'的价值来更新此次估计的表格中当前状态s的价值。

  强化学习中还有很多基本的概念，这里只是一些基础的，大家现在不能完全理解也没关系，到后面就自然会理解了。大家可以通过宝藏李宏毅老师课程上的这张图来大致理解一下强化学习的大致过程以及其中的基本概念。

  ![image-20210105170220452](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/image-20210105170220452.png)

- Sequential Decision Making

![image-20200827110912112](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/image-20200827110912112.png)

- Goal: select actions to maximize total future reward   (最大化未来的累积奖励)。
- Actions may have long term consequences  （动作在很长的时间后才发生作用）。
  Reward may be delayed （奖励存在延时）。
- It may be better to sacrifice immediate reward to gain more long-term reward （最好是牺牲眼前的短暂的奖励去获得更多的长期奖励）。
- Examples:
  - A financial investment (may take months to mature)
  - Refuelling a helicopter (might prevent a crash in several hours)
  - Blocking opponent moves (might help winning chances many moves from now)

#### 全部可观和部分可观

在介绍部分客观和全部可观之前，想先介绍三个概念：

- Environment State： The environment state $S_{t}^{e}$ is the environment's private representation。 environment state指的是对环境信息的一个总结，他可能是一段0101的数字序列，用于产生对应的observation和reward，**他是环境的一种内部的状态**，对于agent来说environment state是不可见的，agent能看到的只有 environment state产生的observation和reward。
- Agent State: The agent state $S_{t}^{a}$ is the agent's internal representation。**是智能体的一些内部表示，比如说强化学习算法的实现**。**他是关于History的一个函数。**
- Information state：包含了history中所有有用的信息。如果它满足马尔科夫性，那么保留了$S_{t},$ 我们就可以将 $S_{1}, \ldots S_{t-1}$ 全部丢掉。

有了这三个概念之后，理解全部客观和部分可观就更简单了：

![image-20200827111022213](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/image-20200827111022213.png)

**Fully Observable Environments**

- 在完全可观察环境下，agent能够直接观察到environment state，也就是说：

  ​                                                                                                                    $O_{t}=S_{t}^{a}=S_{t}^{e}$

- 这类问题被称为MDP问题，Markov decision process。

**Partially Observable Environments**

- 在部分可观察环境中，Observation state 不等于 environment state，我们只能看到部分信息，或者只能看到一些现象，比如在扑克游戏中，我们只能看到公开的牌面，看不到其他人隐藏的牌。

- 这个为题被称为POMDP问题：partially observable Markov decision process。

- 对于这个问题，有几种解决方案：

  - 记住所有的历史状态：$S_{t}^{a}=H_{t}$ 

  - 使用贝叶斯概率，我们可以创建一个Beliefs, $S_{t}^{a}=\left(P\left[S_{t}^{e}=s^{1}\right], \ldots, P\left[S_{t}^{e}=s^{n}\right]\right),$ 这个概率状态的向量组成当前的状态, 所以我们需要保存所 有状交的概率值
  - 使用递归神经网络： $S_{t}^{a}=\sigma\left(S_{t-1}^{a} W_{s}+O_{t} W_{o}\right)$,将之前的状态 $S_{t-1}^{a}$ 与 $W_{s}$ 进行线性组合, 再加上最近的一次观察值的线性組合, 带入非线性函 数 $\sigma$, 得到新的状态

#### Agent的分类

- Types of RL Agents based on What the Agent Learns
  - Value-based agent：我们使用Value function 来计算Policy，Policy是隐含的，不需要直接定义
  - Policy-based agent：每一个状态都映射到一个action，直接使用这个action做决策
  - Actor-Critic agent：综合使用Policy 和 value function，使用Policy，同时保留每次所得到的value function

- Types of RL agent on if there is a model
  - model-free：我们不知道环境的状态概率转移矩阵，不知道环境模型，只是直接根据value, function、policy来直接得出结论
  - model-based：我们知道环境的动态概率转移矩阵，会根据动态概率转移矩阵来预测我们做动作带来的影响从而帮助我们更好地决策

![image-20210311001152751](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210311001152751.png)

#### Exploration和Exploitation

- Exploration: trying new things that might enable the agent to make better decisions in the future
- Exploitation: choosing actions that are expected to yield good reward given the past experience
- Often there may be an exploration-exploitation trade-off.(**When to explore and when to do exploitation？**)
- May have to sacrifice reward in order to explore & learn about potentially better policy
- 利用就是选择最高估计价值的动作而探索不是，短期来看利用是合理的，但是长期来看探索可能会带来总体收益的最大化，探索可以改善对非贪心动作的价值的估计。
- **Exploration(探索)**：倾向于探索环境中新的信息，比如说去没吃过的饭店吃饭。
- **Exploitation(利用)**：倾向于开发使用我们已经探测得到的最大reward，就像我们吃过海底捞了觉得海底捞好吃，以后就什么新的饭店也不去了就只吃海底捞。这么做相对来说确实是“安全”的，起码可以保证结果不至于太坏，但是可能我们永远就吃不到比海底捞更好吃的东西了。

#### Planning和learning

- **Learning problem**：你的环境是未知的，你不能提前知道你的决策将对环境造成什么样的改变。我们需要通过不断地与环境交互，从而得知我们的action造成什么样的改变。

- **Planning Problem**：我们的工作环境是已知的，我们被告知了整个环境的运作规则的详细信息。智能体能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。在这种情况下智能体不用实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。

  **感觉Planning和Learning就像是model-based和model-free？**

#### Prediction和Control

- **预测(Prediction)**:给你一个policy，agent得到，这个policy能够得到多少reward，这是一个预估未来的过程。

- **控制（Control）**: 确定众多决策中，哪一个决策能够得到最多的奖励。

  要强调的是，这两者的区别就在于，预测问题是**给定一个policy**，我们要确定他的value function是多少。而控制问题，是在**没有policy的前提下**，我们要确定最优的value function以及对应的决策方案。

  实际上，这两者是递进的关系，在强化学习中，我们通过解决预测问题，进而解决控制问题：我们会通过先衡量某个策略的好坏再接着对这个策略进行优化。

#### on-policy 和off-policy

​        首先，我们来区分一下Behavior Policy(行为策略) 和Target Policy（目标策略）：

- 行为策略是用来与环境交互产生数据的策略，即在训练过程中实际做决策；

- 而目标策略是通过行为策略采集得到的数据来进行不断地优化、学习的策略，当然，每隔一段时间行为策略会更新成目标策略来保证我们是用优化后的策略来与环境进行交互。

  那么为什么要有两个策略呢？这就是为了解决我们前面讲到的强化学习中的探索与利用的问题：我们**可以利用行为策略来保证探索性，提供多样化的数据；而不断地优化目标策略来保证利用。**

  **On-policy 的目标策略和行为策略是同一个策略**，其好处就是简单粗暴，直接利用枚举就可以优化其策略, 但这样的处理会导致策略其实是在学习一个局部最优, 因为On-policy的策略没办法很好地同时保持即探索又利用; **而Off-policy将目标策略和行为策略分开，**可以在保持探索的同时, 能求到全局最优值。但其难点在于：如何在一个策略下产生的数据来优化另外一个策略? 也可以从Buffer（数据缓冲区）的层面来理解On-policy和Off-policy的不同：**off-policy是将缓冲区数据每次采样出一部分，而on-policy可以看做一次性将缓冲区中所有数据采集出来并删除**。

#### online和offline

​       注意这两个概念和on-policy与off-policy完全没有关系。

- **在线学习：**边玩边学---可能第一关你还没过，你已经学到了不少有用的技术；

- **离线学习：**你没头没脑地（没有学习）玩着游戏，等第一关过了或者死了，然后你仰天长叹，在脑海中回顾着刚刚发生的一切，在这个过程中回顾并学习更新着自己的策略p(action|state)，专业一点的解释可以看这篇[推文](https://www.zhihu.com/question/312824554)。

#### deterministic and stochastic

- Deterministic: given a state, the policy returns a certain action to take：给定一个状态，策略只会返回一个固定的动作。
- Stochastic: given a state, the policy returns a probability distribution of the actions (e.g., $40 \%$ chance to turn left, $60 \%$ chance to turn right) or a certain Gaussian distribution for continuous action。**给定一个状态，策略会返回一个动作的分布**，比如以40%的概率向左走，60%的概率向右走。常用的分布有用于离散动作空间的类别分布（Categorical Distribution）、用于连续动作空间的对角高斯分布（Diagonal Gaussian Distribution）
- A deterministic policy is easily beaten。一个确定性的策略是很容易输的，比如你和同学玩斗地主，每次你都一个炸之后留一张3在手里，那肯定多玩几次你同学看到你只剩一张牌是不会让你走的。

#### Gym中的[环境](https://github.com/openai/gym/tree/master/gym/envs)

强化学习中有很多的benchmark帮助大家很好地去测试自己算法性能，像Gym、MoJoCo等，MoJoCo是需要付费的，学生可以试用一年。可以一步一步来，先给大家介绍一下最简单的Gym环境，Gym里面其实有很多的环境可以供大家去使用，具体的一些细节我觉得大家最好就上Gym的[gitlab](https://github.com/openai/gym/tree/master/gym/envs)上看就好了，这里主要是给大家讲一些基础的。

安装的话也很简单，就一个命令

```bash
pip install gym
```

或者用conda装也行，其实也是一行命令，把pip换成conda。

windows环境下用anaconda[安装gym](https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30)

gym的最基本的使用可以看我下面的几行代码，都有注释应该比较好理解。

- 基本使用

  ```python
  import gym
  
  env=gym.make("MountainCar-v0") #创建对应的游戏环境
  env.seed(1) # 可选，设置随机数，以便让过程重现
  env=env.unwrapped  # 可选，为环境增加限制，对训练有利
  
  # ----------------------动作空间和状态空间---------------------#
  print(env.action_space)  # 动作空间，输出的内容看不懂
  print(env.action_space.n)  # 当动作是离散的时，用该方法获取有多少个动作
  # env.observation_space.shape[0] # 当动作是连续的时，用该方法获取动作由几个数来表示
  print(env.action_space.low)  # 动作的最小值
  print(env.action_space.high)  # 动作的最大值
  print(env.action_space.sample())  # 从动作空间中随机选取一个动作
  # 同理，还有 env.observation_space，也具有同样的属性和方法（.low和.high方法除外）
  #-------------------------------------------------------------#
  
  for episode in range(100): #每个回合
      s=env.reset()  # 重新设置环境，并得到初始状态
      while True:  # 每个步骤
          env.render()  # 展示环境
          a=env.action_space.sample() # 智能体随机选择一个动作
          s_,r,done,info=env.step(a)   # 环境返回执行动作a后的下一个状态、奖励值、是否终止以及其他信息
          if done:
              break
  ```

  下面以CartPole为例，当大家要用一个以前从来没有用过的新环境时，首先脑子里要大概有上面的强化学习的一套代码：

  - 创建环境、
  - 设置环境随机种子、
  - 初始化环境、
  - 给环境传动作并获取环境状态、
  - 判断是不是终止...... 

  大概都会这么一些步骤。当然每个环境的调用是有差别的，比如说要知道他的动作空间是什么样的。

  一般gym中的动作空间会有这几种：

  - `Box`: A N-dimensional box that contains every point in the action space.
  - `Discrete`: A list of possible actions, where each timestep only one of the actions can be used.
  - `MultiDiscrete`: A list of possible actions, where each timestep only one action of each discrete set can be used.
  - `MultiBinary`: A list of possible actions, where each timestep any of the actions can be used in any combination.

  他的observation_space是怎么样的，像这些就直接去环境的源代码中看就好了，看一下文档注释大概就知道了。比如下面的CartPole中的文档注释：

- [CartPole](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)

  ```python
  """
  Description:
      A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.
      一根杆子通过一个未驱动的接头连接到小车上，小车沿着无摩擦的轨道移动。钟摆开始直立，目标是通过增加或减少小车的速度来防止它倒下。
  
  Source:
      This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson
  
  Observation: 
      Type: Box(4)
      Num	Observation                 Min         Max
      0	Cart Position             -4.8            4.8
      1	Cart Velocity             -Inf            Inf
      2	Pole Angle                 -24 deg        24 deg
      3	Pole Velocity At Tip      -Inf            Inf
      
  Actions:
      Type: Discrete(2)
      Num	Action
      0	Push cart to the left
      1	Push cart to the right
      
      Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it
  
  Reward:
      Reward is 1 for every step taken, including the termination step
  
  Starting State:
      All observations are assigned a uniform random value in [-0.05..0.05]
  
  Episode Termination:
      Pole Angle is more than 12 degrees
      Cart Position is more than 2.4 (center of the cart reaches the edge of the display)
      Episode length is greater than 200
      Solved Requirements
      Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.
  """
  ```

  首先看了介绍就大概知道CartPole游戏是一个小车连着一根杆子，然后小车通过控制左右移动来防止杆子倒下。

  然后他下面还告诉了我们CartPole游戏中observation返回的四个值的含义：小车的位置、小车的速度、杠的角度、杠的尖端的速度；以及action_spaces里面就是一个0，1，如果我们给step函数传0，小车就往左走；传1小车就往右走。然后还告诉了我们每坚持1步不倒，就会得到一个+1的reward；还告诉了我们环境的一些终止条件，比如说如果能坚持200步不倒环境就会终止。

  有了这些信息之后，我们可以先简单地写一些测试的代码,就看一下环境到底是怎么样的，动作要怎么传，observation到底是什么样的，环境render之后是什么样的。

  ```python
  import gym
  
  env = gym.make("CartPole-v0")
  env.seed(1)
  # ----------------------动作空间和状态空间---------------------#
  print("action space:", env.action_space)  # 动作空间
  print("The number of the action:", env.action_space.n)  # 当动作是离散的时，用该方法获取有多少个动作
  for i in range(5):
      print("The testing action:", env.action_space.sample())  # 从动作空间中随机选取一个动作,可以多打印几次，看看动作空间都是什么样的
  print(env.observation_space)  # 状态空间
  print(env.observation_space.shape)  # 状态空间的形状
  for i in range(5):
      print("The testing observation:", env.observation_space.sample())  # 对状态空间进行采样，打印出来看一下
  
  for episode in range(100):  # 每个回合
      s = env.reset()  # 重新设置环境，并得到初始状态
      while True:  # 每个步骤
          if episode % 50 == 0:
              env.render()  # 展示环境
          a = env.action_space.sample()  # 智能体随机选择一个动作
          s_, r, done, info = env.step(a)  # 环境返回执行动作a后的下一个状态、奖励值、是否终止以及其他信息
          if done:
              break
  
  ```

  然后看一下结果：我们大概就知道环境大致是什么样的，它的动作空间和状态空间大概是什么量级。

  ![image-20210311000900265](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210311000900265.png)

  

  ![image-20210311001023124](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210311001023124.png)

  

- [Frozen Lake](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py)

```python
 """
    Winter is here. You and your friends were tossing around a frisbee at the park
    when you made a wild throw that left the frisbee out in the middle of the lake.
    The water is mostly frozen, but there are a few holes where the ice has melted.
    If you step into one of those holes, you'll fall into the freezing water.
    At this time, there's an international frisbee shortage, so it's absolutely imperative that
    you navigate across the lake and retrieve the disc.
    However, the ice is slippery, so you won't always move in the direction you intend.
    The surface is described using a grid like the following

        SFFF
        FHFH
        FFFH
        HFFG

    S : starting point, safe
    F : frozen surface, safe
    H : hole, fall to your doom
    G : goal, where the frisbee is located

    The episode ends when you reach the goal or fall in a hole.
    You receive a reward of 1 if you reach the goal, and zero otherwise.

    """
```

同理Frozen Lake也是一样的去看源码中的注释。

#### 强化学习的一些资源

这里主要给大家分享一些课程，书籍还有代码库。

##### 课程

- [CS 285 at UC Berkeley](https://www.youtube.com/watch?v=7-D8RL3D6CI&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=32)，这个课程是2020年底Sergey Levine大神在UC Berkeley教的课程，我觉得讲的特别详细，而且里面加了很多一些最新的研究成果。

- [David Silver’s course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)，这个课程是David Sliver在2015年出的课程，非常的经典，相信很多入门强化学习的同学都是看的这个课程，里面的很多东西课程刚开始听会比较难懂，可能得多听几遍。

- [Berkeley’s Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)：这个课我其实没看过，但是看了一下lectures，是一群大佬讲的这个课程，质量应该也很好。

- [Stanford CS234: Reinforcement Learning](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) ：这个课暂时没看过，不过看评价感觉挺好的。

- [Stanford CS330: Multi-Task and Meta-Learning, 2019](https://www.youtube.com/watch?v=0rZtSwNOTQo&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5) ：Finn关于多任务学习和元学习的课程，强推。

- [李宏毅老师的机器学习课程](https://speech.ee.ntu.edu.tw/~tlkagk/courses.html)：李宏毅老师的机器学习课程里面有一些也涉及到了强化学习的内容，李宏毅老师是用中文讲的课程，而且总是能把一个比较复杂的问题以一种很简单的方式讲出来。

- 港中文周博磊老师的课程：我觉得大家如果看英文课比较吃力的话建议大家可以先看周博磊老师的课程先入个门，这个课程的内容相对比较简单，比较好入门。

  - [B站视频](https://space.bilibili.com/511221970/channel/detail?cid=105354)
  - [课程作业](https://github.com/cuhkrlcourse/ierg6130-assignment)
  - [RL demo代码](https://github.com/cuhkrlcourse/RLexample)

  课程的话暂时就只想到这么多，上面的这些课程我听了的大部分都记了笔记，大部分是直接记在课件上的，**大家如果有想要的也可以留言告诉我**。

##### 书籍

- Sutton的强化学习书籍：这个算是强化学习里面最经典的书了，Sutton巨佬的作品，基本上不管是哪个老师上强化学习的课程都会推荐这本书，不过这本书里面将的大多数都是value-based的方法。
  - [英文版](http://incompleteideas.net/book/the-book-2nd.html)
  - [中文版](https://rl.qiwihui.com/zh_CN/latest/preface2nd.html)，这个翻译其实还存在挺多问题的，有些语句和专业词汇翻译的也不太好，建议大家可以购买俞凯老师翻译的纸质版书籍。
  - [代码](http://incompleteideas.net/book/code/code2nd.html)
  - [书中的提到的文献](https://drive.google.com/drive/folders/1V9jAShWpccLvByv5S1DuOzo6GVvzd4LV)
  - [sliders](https://drive.google.com/drive/folders/0B3w765rOKuKANmxNbXdwaE1YU1k)
- [Algorithms for Reinforcement Learning, Szepesvari](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.549&rep=rep1&type=pdf)：很多经典的强化学习算法里面都有。
- [《强化学习精要：核心算法与TensorFlow 实现》](https://book.douban.com/subject/30214611/)：这本书是我偶然在实验室的一个书柜里面找到的，随手翻了一下发现写的挺好的，这本书和Sutton的书是我平时用的最多的了。

##### 代码库

- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)

- [Baselines](https://github.com/openai/baselines)

- [Stable-Baselines](https://github.com/hill-a/stable-baselines)

- [Ray/Rlib](Ray/Rlib)

- [Pytorch-DRL](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch)

- [rlpyt](https://github.com/astooke/rlpyt)

- [Tianshou](https://github.com/thu-ml/tianshou)

  这些库有些有tensorflow写的，有些用pytorch写的，具体的对比和测评可以看Tianshou的作者写的这个[测评](https://tianshou.readthedocs.io/zh/latest/docs/4-exp.html)。

#### 强化学习的学习建议

下面的很多建议我都是从Stable Baselines 上面的[tips](https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html)总结的，然后加上了自己一些粗浅的看法。

- Read about RL and Stable Baselines ：多看一些强化学习算法的Baseline代码，比如我上面提供的代码库

- Do quantitative experiments and hyperparameter tuning if needed. This factor, among others, explains that results in RL may vary from one run to another (i.e., when only the seed of the pseudo-random generator changes). For this reason, **you should always do several runs to have quantitative results**. ： 因为强化学习算法跑出来的结果有时候会差特别多，所以应该多跑几组实验得到量化的结果。

- Good results in RL **are generally dependent on finding appropriate hyperparameters**. Recent algorithms (PPO, SAC, TD3) normally require little hyperparameter tuning, however, *don’t expect the default ones to work* on any environment. 虽然最近的一些算法比如PPO、SAC、TD3不太需要调参，但是不要期待一个默认的超参数设置可以在所有环境下都能用，所有强化学习中的一些好的结果都需要一些超参数的调节。

- When applying RL to a custom problem, you should always normalize the input to the agent (e.g. using VecNormalize for PPO2/A2C) and look at common preprocessing done on other environments (e.g. for [Atari](https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/), frame-stack, …). Please refer to *Tips and Tricks when creating a custom environment* paragraph below for more advice related to custom environments.  应该根据智能体的不同，对输入进行规范化。

- Evaluate the performance using a separate test environment  ：有时候训练的时候会加入一些epsilon贪婪等来提升探索，但是在测试的时候应该用一个分开的环境直接对模型进行测试。

- As a general advice, to obtain better performances, you should augment the budget of the agent (number of training timesteps). 为了更好的效果，训练的久一点。

- In order to achieve the desired behavior, expert knowledge is often required to design an adequate reward function. This *reward engineering* (or *RewArt* as coined by [Freek Stulp](http://www.freekstulp.net/)), necessitates several iterations. As a good example of reward shaping, you can take a look at [Deep Mimic paper](https://xbpeng.github.io/projects/DeepMimic/index.html) which combines imitation learning and reinforcement learning to do acrobatic moves. 加入一些专家经验去设计比较好的奖励。

- Reproducibility：Completely reproducible results are not guaranteed across Tensorflow releases or different platforms. Furthermore, results need not be reproducible between CPU and GPU executions, even when using identical seeds.In order to make computations deterministic on CPU, on your specific problem on one specific platform, you need to pass a `seed` argument at the creation of a model and set n_cpu_tf_sess=1 (number of cpu for Tensorflow session). If you pass an environment to the model using set_env(), then you also need to seed the environment first.设置随机种子增强结果的复现性。

- 没有哪个算法会适合所有的任务，那么平时怎么选择算法呢？ 首先第一个标准就是看看问题的动作空间是离散的还是连续的，比如像DQN只适合离散动作，SAC只适合连续的动作；还有就是你要不要并行化你的训练：具体怎么选看下面：

  - Discrete Actions - Single Process： DQN with extensions (double DQN, prioritized replay, …) and ACER are the recommended algorithms. DQN is usually slower to train (regarding wall clock time) but is the most sample efficient (because of its replay buffer).**离散动作、不并行化**：DQN及其扩展，ACER等。DQN训练很慢，但是因为有replay buffer的存在样本利用率比较高。

    - Discrete Actions - Multiprocessed： You should give a try to PPO2, A2C and its successors (ACKTR, ACER). If you can multiprocess the training using MPI, then you should checkout PPO1 and TRPO.  **离散动作、并行化**：PPO2、A2C及其对其改进（ACKER、ACER）等

    - Continuous Actions - Single Process： Current State Of The Art (SOTA) algorithms are SAC and TD3. Please use the hyperparameters in the RL zoo for best results.  **连续动作、不并行化**：SAC、TD3。

    - Continuous Actions - Multiprocessed： Take a look at PPO2, TRPO or A2C. Again, don’t forget to take the hyperparameters from the RL zoo for continuous actions problems (cf Bullet envs). If you can use MPI, then you can choose between PPO1, TRPO and DDPG. 连续动作、并行化: SAC、TD3。

- 在创建一个自定义的环境的时候也有一些tricks：

  - always normalize your observation space when you can, i.e., when you know the boundaries   规范化你的状态空间。
  - normalize your action space and make it symmetric when continuous (cf potential issue below).  A good practice is to rescale your actions to lie in [-1, 1]. This does not limit you as you can easily rescale the action inside the environment  规范化你的动作空间，比如把你的动作缩放到[-1, 1]之间。
  - start with shaped reward (i.e. informative reward) and simplified version of your problem  使用一些shaped reward，开始的时候使用问题的简单形式。
  - debug with random actions to check that your environment works and follows the gym interface: 用随机动作去测试你的环境是否正常工作。

- 实现强化学习算法的一些trick：

  - Read the original paper several times  多看几遍原始论文。

  - Read existing implementations (if available)  看算法的现有的实现。

  - Try to have some “sign of life” on toy problems   先在简单的问题试起来。

  - Validate the implementation by making it run on harder and harder envs (you can compare results against the RL zoo). You usually need to run hyperparameter optimization for that step. You need to be particularly careful on the shape of the different objects you are manipulating (a broadcast mistake will fail silently cf issue #75) and when to stop the gradient propagation   通过在越来越难的环境中运行来验证实现(可以将结果与RL zoo进行比较)

    下面列举了哪些环境是比较简单的，哪些环境是比较难的。

  A personal pick (by @araffin) for environments with gradual difficulty in RL with continuous actions:

  - **Pendulum (easy to solve)**
  - **HalfCheetahBullet (medium difficulty with local minima and shaped reward)**
  - **BipedalWalkerHardcore (if it works on that one, then you can have a cookie)**

  in RL with discrete actions:

  -  **CartPole-v1 (easy to be better than random agent, harder to achieve maximal performance)**
  -  **LunarLander**
  -  **Pong (one of the easiest Atari game)**
  -  **other Atari games (e.g. Breakout)**

### 第二章- MDP、K臂老虎机及贝尔曼方程

#### Markov Decision Process

##### Markov Property

State $s_{t}$ is Markovian if and only if:
$$
\begin{aligned}
p\left(s_{t+1} \mid s_{t}\right) &=p\left(s_{t+1} \mid h_{t}\right) \\
p\left(s_{t+1} \mid s_{t}, a_{t}\right) &=p\left(s_{t+1} \mid h_{t}, a_{t}\right)
\end{aligned}
$$
The future is independent of the past given the present. 未来的情况只和当前状态有关，和再之前的状态无关。

##### Markov Reward Process

![image-20200827185803866](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/image-20200827185803866.png)

##### Return and Value function

- Horizon的定义：一个回合内的最大时间步

- Return的定义：$G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} R_{t+4}+\ldots+\gamma^{T-t-1} R_{T}$

- Value function的定义（表示未来奖励的值）：

  ​                                                                               $\begin{aligned} V_{t}(s) &=\mathbb{E}\left[G_{t} \mid s_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots+\gamma^{T-t-1} R_{T} \mid s_{t}=s\right] \end{aligned}$

##### 为什么要有$\gamma$：

- 避免在循环的马尔可夫过程中有无限的return
- 关于未来的不确定性不能完全地展示，那这个$\gamma$其实就表征了一定的不确定性
- 比起未来的奖励人类更偏爱即时奖励
- 如果$\gamma=0$,表示我们只关心立即奖励而不关心未来的奖励；如果$\gamma=1$,表示我们觉得未来的奖励和现在的即时奖励一样重要，所以就可以把$\gamma$作为超参数来得到不同行为的agent

##### Markov Descion Process

![image-20200827194057887](C:\Users\yunhu\Desktop\RL_code_from_scratch\强化学习的基本介绍.assets\image-20200827194057887.png)

##### MDP中的重要定义

- 在有限MDP中, 状态, 动作和奖励$\quad(\mathcal{S}, \mathcal{A}$ 和 $\mathcal{R})$ 的集合都具有有限数量的元素。 在这种情况 下，随机变量 $R_{t}$ 和 $S_{t}$ 具有明确定义的离散概率分布, 仅取决于先前的状态和动作。 也就是说对于这些随机变量的特定值, $s^{\prime} \in \mathcal{S}$ 和 $r \in \mathcal{R}, \quad$在给定前一状态和动作的特定值的情况下，存在这些值在时间t发生的概率：
  $$
  p\left(s^{\prime}, r \mid s, a\right) \doteq \operatorname{Pr}\left\{S_{t}=s^{\prime}, R_{t}=r \mid S_{t-1}=s, A_{t-1}=a\right\}
  $$

- $p: \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow[0,1]$ 是四个参数的普通确定性函数。$\quad$但这里只是提醒我们 $p$ 指定 $s$ 和 $a$ 的每个选择的概率分布, 即对所有 $s \in \mathcal{S}, a \in \mathcal{A}(s)$
  $\sum_{s^{\prime} \in \mathcal{S}} \sum_{r \in \mathcal{R}} p\left(s^{\prime}, r \mid s, a\right)=1$。

- 从四参数动力学函数p中，可以计算出人们可能想知道的关于环境的任何其他信息，例如状态转移概率 (我们将其略微滥用符号表示为三参数函数 $p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow[0,1])$,
  $$
  p\left(s^{\prime} \mid s, a\right) \doteq \operatorname{Pr}\left\{S_{t}=s^{\prime} \mid S_{t-1}=s, A_{t-1}=a\right\}=\sum_{r \in \mathcal{R}} p\left(s^{\prime}, r \mid s, a\right)
  $$

- 我们还可以将状态 - 动作对的预期奖励计算为双参数函数 $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$
  $$
  r(s, a) \doteq \mathbb{E}\left[R_{t} \mid S_{t-1}=s, A_{t-1}=a\right]=\sum_{r \in \mathcal{R}} r \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime}, r \mid s, a\right)
  $$

- 以及状态 - 行动 - 下一状态三元组的预期奖励作为三个参数函数 $r: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$
  $$
  r\left(s, a, s^{\prime}\right) \doteq \mathbb{E}\left[R_{t} \mid S_{t-1}=s, A_{t-1}=a, S_{t}=s^{\prime}\right]=\sum_{r \in \mathcal{R}} r \frac{p\left(s^{\prime}, r \mid s, a\right)}{p\left(s^{\prime} \mid s, a\right)}
  $$

##### MRP和MDP的对比

MDP相当于是在MRP的中间加了一层决策层，相当于MRP是一个随波逐流的过程，但是MDP是一个有agent在掌舵的过程。

![image-20200827195203392](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200827195203392.png)

#### K臂老虎机介绍及其Python实现

如果大家想对K臂老虎机做一个比较深入的了解的话，建议大家去阅读这篇[博客](https://zhuanlan.zhihu.com/p/52727881)，作者写的挺清楚的，而且还推荐了很多的其他材料，我这里主要是对K臂老虎机做一个简要的介绍。

##### 定义

K臂老虎机（Multi-armed bandit，简称MAB）最早的场景是在赌场里面。赌场里面有K台老虎机，每次去摇老虎机都需要一个代币，且老虎机都会以一定概率吐出钱，你手上如果有T个代币，也就是你一共可以摇T次，你怎么才能使你的期望回报最大？当然我们要先假设每个老虎机吐钱的概率是不一样的，不然你怎么摇就都是一样的了。

![image-20210114210246878](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210114210246878.png)

我们一般也将所有的下面这种形式的问题成为K臂老虎机问题:  你可以反复面对 k 种不同的选择或行动。在每次选择之后，你会收到一个数值奖励，该奖励取决于你选择的行动的固定概率分布。 你的目标是在一段时间内最大化预期的总奖励。

**如果我们是贝叶斯人，我们在实际对老虎机进行操作之前其实对老虎机吐钱的概率就已经有了一个先验的分布，然后我们不断地进行试验，根据试验的结果取调整我们前面的分布；而如果我们是频率学家，那我们一开始对这些机器吐钱的概率其实是没有先验的，我们会通过实验去预测出每台机器吐钱的概率，然后根据这个概率去不断优化我们的决策。**

但不管从哪种角度出发，**K臂老虎机的问题其实就是一个探索与利用的问题**，就比如说我们先进行来m次实验（m<T)，发现了第一个臂吐钱的频率更高，那接下来我们是一直去摇第一个臂（利用：exploitation）还是说我们还去试着摇一下其他的臂（探索：exploration），从短期来看利用是好的，但是从长期来看探索是好的。

##### 基本概念

在我们的K臂老虎机中，只要选择了该动作, $k$ 个动作的每一个都有预期的或平均的奖励, 让我们 称之为该行动的 价值。我们将在时间步 $t$ 选择的动作表示为 $A_{t},$ 并将相应的奖励表示为 $R_{t_{}}$。 然后, 对于任意动作 $a$ 的价值, 定义 $q_{*}(a)$ 是给定 $a$ 选择的预期奖励：
$$
q_{*}(a) \doteq \mathbb{E}\left[R_{t} \mid A_{t}=a\right]
$$
如果我们知道每个动作的价值, 那么解决 K臂老虎机将是轻而易举的：你总是选择具有最高价值的动作。但是**我们不知道实际动作价值, 尽管你可能有估计值**。 我们将在时间步骤$t$ 的动作 $a$ 的估计值表示为 $Q_{t}(a)$ 。 我们希望 $Q_{t}(a)$ 接近 $q_{*}(a)$ 。

##### K臂老虎机的变种

我们在上面定义中介绍的K臂老虎机其实是最简单的一种场景，K臂老虎机还有很多其他的变形：

- 如果那些臂的吐钱的概率分布在一开始就设定好了，而且之后不再改变，则称为**oblivious adversary setting**。
- 如果那些臂吐钱的概率设定好了之后还会发生变化，那么称为**adaptive adversary setting**。
- 如果把待推荐的商品作为MAB问题的arm，那么我们在推荐系统中我们就还需要考虑用户作为一个活生生的个体本身的兴趣点、偏好、购买力等因素都是不同的，也就是我们需要考虑同一臂在不同上下文中是不同的, 这种也被称为**contextual bandits**。
- 如果每台老虎机每天摇的次数有上限，那我们就得到了一个**Bandit with Knapsack**问题。

##### greedy和$\epsilon-greedy$

greedy（贪婪）的算法也就是选择具有最高估计值的动作之一：$A_{t}=\underset{a}{\operatorname{argmax}} Q_{t}(a)$，也就是相当于我们只做exploitation ; 而$\epsilon-greedy$以较小的概率 $\epsilon$地从具有相同概率的所有动作中随机选择, 相当于我们在做exploitation的同时也做一定程度的exploration。greedy的算法很容易陷入执行次优动作的怪圈，当reward的方差更大时，我们为了做更多的探索应该选择探索度更大的$\epsilon-greedy$，但是当reward的方差很小时，我们可以选择更greedy的方法，在实际当中我们很多时候都会让$\epsilon$ 从一个较大的值降低到一个较小的值，比如说从1降低到0.1，相当于我们在前期基本上只做探索，后期只做利用。

- Trade-off between exploration and exploitation

- $\epsilon$ -Greedy Exploration: Ensuring continual exploration
  - Q All actions are tried with non-zero probability
  - With probability $1-\epsilon$ choose the greedy action
  - B With probability $\epsilon$ choose an action at random

$$
\pi(a \mid s)=\left\{\begin{array}{ll}
\epsilon /|\mathcal{A}|+1-\epsilon & \text { if } a^{*}=\arg \max _{a \in \mathcal{A}} Q(s, a) \\
\epsilon /|\mathcal{A}| & \text { otherwise }
\end{array}\right.
$$

Policy improvement theorem: For any $\epsilon$ -greedy policy $\pi,$ the $\epsilon$ -greedy policy $\pi^{\prime}$ with respect to $q_{\pi}$ is an improvement, $v_{\pi^{\prime}}(s) \geq v_{\pi}(s)$
$$
\begin{aligned}
q_{\pi}\left(s, \pi^{\prime}(s)\right) &=\sum_{a \in \mathcal{A}} \pi^{\prime}(a \mid s) q_{\pi}(s, a) \\
&=\frac{\epsilon}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} q_{\pi}(s, a)+(1-\epsilon) \max _{a} q_{\pi}(s, a) \\
& \geq \frac{\epsilon}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} q_{\pi}(s, a)+(1-\epsilon) \sum_{a \in \mathcal{A}} \frac{\pi(a \mid s)-\frac{\epsilon}{|\mathcal{A}|}}{1-\epsilon} q_{\pi}(s, a) \\
&=\sum_{a \in \mathcal{A}} \pi(a \mid s) q_{\pi}(s, a)=v_{\pi}(s)
\end{aligned}
$$
Therefore, $v_{\pi^{\prime}}(s) \geq v_{\pi}(s)$ from the policy improvement theorem

MC with    $$\epsilon-\text { Greedy Exploration }$$![image-20200915094903496](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200915094903496.png)

##### softmax 方法

softamx是另一种兼顾探索与利用的方法，它既不像greedy算法那样贪婪，也没有像$\epsilon-$ greedy那样在探索阶段做随机动作而是使用 softmax函数计算每一个arm被选中的概率，以更高的概率去摇下平均收益高的臂，以更地的概率去摇下平均收益低的臂。 $a r m_{i}$ 表示第i 个手柄, $\quad U_{i}$ 表示手柄的平均收 益, k是手柄总数。
$$
p\left(a r m_{i}\right)=\frac{e^{u_{i}}}{\sum_{j}^{k} e^{u_{i}}}
$$
当然这里有一个问题是为什么要用softmax，我们直接用某一个臂得到的平均收益除以总的平均收益不行吗？**我理解上感觉softmax方法是在agrmax方法和直接除这种方法之间的方法**，因为softmax加上e之后其实会让平均收益低的臂和平均收益高的臂走向极端，**也就是让策略越来越激进，甚至到最终收敛成argmax**？而且我感觉图像分类里面经常用softmax一方面是因为求梯度比较好计算，另一方面是因为有时候softmax之前得到的分数可能有负数，那我们这里的好处好可以加上就是刚开始某一个臂的平均收益是0的时候我们依旧会有一定概率选它而不会像下面公式里面的这种一样不选它。
$$
p\left(a r m_{i}\right)=\frac{{u_{i}}}{\sum_{j}^{k} {u_{i}}}
$$
所以总的来说softmax有三个好处：

- 便于求梯度
- 在刚开始某一个臂收益为0的时候这个臂依旧有被选上的可能
- softma**x算法让平均收益低的臂和平均收益高的臂走向极端，也就是让策略越来越激进，甚至到最终收敛成argmax**，就有点像$\epsilon-greedy$中$\epsilon$不断下降一样。

##### 一个简单的赌博机算法

![image-20200828094404629](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200828094404629.png)

循环的最后一步其实用到了

​                                                                                                         $$\begin{aligned} Q_{n+1} &=\frac{1}{n} \sum_{i=1}^{n} R_{i} \\ &=\frac{1}{n}\left(R_{n}+\sum_{i=1}^{n-1} R_{i}\right) \\ &=\frac{1}{n}\left(R_{n}+(n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_{i}\right) \\ &=\frac{1}{n}\left(R_{n}+(n-1) Q_{n}\right) \\ &=\frac{1}{n}\left(R_{n}+n Q_{n}-Q_{n}\right) \\ &=Q_{n}+\frac{1}{n}\left(R_{n}-Q_{n}\right) \end{aligned}$$

也就是：新估计←旧估计+步长[目标−旧估计]

##### Python 代码实现

在代码里面实现了$\epsilon-greedy$、softmax，以及直接根据当前各个臂的平均收益去决策三种方法，完整的代码放在github上了，写的比较匆忙，后面会再更新一下放到github的[仓库](https://github.com/Yunhui1998/How-do-I-learn-RL)之中

```python
# 作者：曾云辉
# 创建时间：2021/1/13 23:54
# IDE：PyCharm 
# encoding: utf-8

import random
import math
import numpy as np
import matplotlib.pyplot as plt

ARM_NUM = 5
E_GREEDY_FACTOR = 0.9
SEED = random.randint(1, 10000)
TEST_STEPS = 1000


class MAB:
    def __init__(self, arm_num: int) -> None:
        """
        :param arm_num:  the number of arms  臂的数量
        """
        self.arm_num = arm_num    # 设置臂的数量
        self.probability = dict({})  # 设置每个臂能摇出一块钱的概率
        self.try_time = dict({})  # 每个臂已经摇过的次数
        self.reward = dict({})  # 每个臂已经获得的钱
        self.reward_all = 0  # 所有臂获得的收益之和
        self.try_time_all = 0  # 总的尝试的次数

    def reset(self, seed: int) -> None:
        """
        Each arm is initialized, and each arm is set the same when passing in the same random seed
        对每一个臂进行初始化，当传入的随机种子一样时，每个臂的设置相同
        :param seed: random seed  传入的随机种子
        """
        print("We have %d arms" % self.arm_num)
        for num in range(self.arm_num):
            random.seed(num+seed)
            self.probability[str(num + 1)] = random.random()
            self.try_time[str(num + 1)] = 0
            self.reward[str(num + 1)] = 0

    def step(self, arm_id: str):
        """
        Change the arm according to the arm_id
        当传入每次要摇下的臂的编号后，老虎机的状态发生变化
        :param arm_id: the id of the arm in this step 这一步控制摇下杆的id
        """
        self.try_time[arm_id] += 1
        self.try_time_all += 1
        if random.random() < self.probability[arm_id]:
            self.reward[arm_id] += 1
            self.reward_all += 1

    def render(self):
        """
           draw the multi-armed bandit,including tried times and reward
           for each arm, and total tried times and rewards.
        """
        if self.arm_num <= 10:
            print('*' * 8 * (self.arm_num + 1) + '**')
            title = str(self.arm_num) + " arm bandit"
            title_format = '{:^' + str(8 * (self.arm_num + 1)) + 's}'
            print('*' + title_format.format(title) + '*')

            print('*' + ' ' * 8 * (self.arm_num + 1) + '*')

            print('*{:^8s}'.format('arm'), end='')
            for arm in range(self.arm_num):
                print('{:^8d}'.format(arm + 1), end='')
            print('*\n')

            print('*{:^8s}'.format('tried'), end='')
            for arm in range(self.arm_num):
                print('{:^8d}'.format(self.try_time[str(arm + 1)]), end='')
            print('*\n')

            print('*{:^8s}'.format('reward'), end='')
            for arm in range(self.arm_num):
                print('{:^8d}'.format(self.reward[str(arm + 1)]), end='')
            print('*\n')

            print('*' + title_format.format("total tried:" + str(self.try_time_all)) + '*')
            print('*' + title_format.format("total rewards:" + str(self.reward_all)) + '*')
            print('*' + ' ' * 8 * (self.arm_num + 1) + '*')
            print('*' * 8 * (self.arm_num + 1) + '**')


def e_greedy_method(mab):
    """
       e greedy method: define a e_greedy_factor and create a random number,
       when the random number is less then e_greedy_factor, then pick a arm
       randomly, else pick the arm with argmax q_table.
       :param mab: the class MBA
       :return: selected arm_id
    """
    q_table = []
    for arm_num in range(mab.arm_num):
        if mab.try_time[str(arm_num+1)] != 0:
            q_table.append(mab.reward[str(arm_num+1)]/mab.try_time[str(arm_num+1)])
        else:
            q_table.append(0)
    if random.random() < E_GREEDY_FACTOR:
        arm_id = random.randint(1, mab.arm_num)
    else:
        arm_id = np.argmax(q_table) + 1
    return arm_id


def sofmax_method(mab):
    """
    softmax method: calculate the softmax value of each arm's avarage reward,
    and pick the arm with greatest softmax value.
    :param mab: the class MBA
    :return: selected arm_id
    """
    exp_sum = 0
    softmax_list = []

    for arm_num in range(mab.arm_num):
        if mab.try_time[str(arm_num+1)] > 0:
            exp_sum += math.exp(mab.reward[str(arm_num+1)] / mab.try_time[str(arm_num+1)])
        else:
            exp_sum += math.exp(0)
    assert exp_sum > 0
    for arm_num in range(mab.arm_num):
        if mab.try_time[str(arm_num+1)] == 0:
            avg_reward_temp = 0
        else:
            avg_reward_temp = mab.reward[str(arm_num+1)] / mab.try_time[str(arm_num+1)]
        softmax_list.append(math.exp(avg_reward_temp) / exp_sum)
    arm_id = np.random.choice(mab.arm_num, 1, p=softmax_list)[0]
    print("The softmax list is", softmax_list)
    print("The id of returned arm is ", arm_id+1)
    return arm_id + 1


def average_method(mab):
    """
    decide the arm_id according to the average return of each arm but don't do the math.exp() operation like softmax
    :param mab: the class MBA
    :return: selected arm_id
    """
    sum_average = 0
    softmax_list = []

    for arm_num in range(mab.arm_num):
        if mab.try_time[str(arm_num + 1)] > 0:
            sum_average += (mab.reward[str(arm_num + 1)] / mab.try_time[str(arm_num + 1)])
        else:
            sum_average += 0
    if sum_average == 0:
        arm_id = np.random.choice(mab.arm_num) + 1
    else:
        for arm_num in range(mab.arm_num):
            if mab.try_time[str(arm_num + 1)] == 0:
                avg_reward_temp = 0
            else:
                avg_reward_temp = mab.reward[str(arm_num + 1)] / mab.try_time[str(arm_num + 1)]
            softmax_list.append(avg_reward_temp / sum_average)
        arm_id = np.random.choice(mab.arm_num, 1, p=softmax_list)[0]
    print("The softmax list is", softmax_list)
    print("The id of returned arm is ", arm_id + 1)
    return arm_id + 1


if __name__ == '__main__':
    reward_list = []
    mab_test = MAB(ARM_NUM)
    print("****Multi-armed Bandit***")
    mab_test.reset(SEED)
    mab_test.render()
    for i in range(TEST_STEPS):
        mab_test.step(str(average_method(mab_test)))
        reward_list.append(mab_test.reward_all/mab_test.try_time_all)
        if (i+1) % 20 == 0:
            print("We have test for %i times" % (i+1))
            mab_test.render()
    plt.plot(reward_list)
    plt.show()

```



#### Bellman Equation

贝尔曼方程定义了状态之间的迭代关系，是强化学习里面它特别重要的一个知识点。

##### V(s)

价值函数其实从一个更长远的角度定义的一个状态的好坏，价值函数不仅仅考虑了短期的即时奖励，更重要的是价值函数考虑了到达这个状态会带来的长期的效益，因此相比于reward是一个更长远地衡量状态的方式。

$V(s)=R(s)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)$ （第一部分可以看作立即奖励，第二部分可以看作未来的奖励）

​                                                      $\left[\begin{array}{c}V\left(s_{1}\right) \\ V\left(s_{2}\right) \\ \vdots \\ V\left(s_{N}\right)\end{array}\right]=\left[\begin{array}{c}R\left(s_{1}\right) \\ R\left(s_{2}\right) \\ \vdots \\ R\left(s_{N}\right)\end{array}\right]+\gamma\left[\begin{array}{cccc}P\left(s_{1} \mid s_{1}\right) & P\left(s_{2} \mid s_{1}\right) & \ldots & P\left(s_{N} \mid s_{1}\right) \\ P\left(s_{1} \mid s_{2}\right) & P\left(s_{2} \mid s_{2}\right) & \ldots & P\left(s_{N} \mid s_{2}\right) \\ \vdots & \vdots & \ddots & \vdots \\ P\left(s_{1} \mid s_{N}\right) & P\left(s_{2} \mid s_{N}\right) & \ldots & P\left(s_{N} \mid s_{N}\right)\end{array}\right]\left[\begin{array}{c}V\left(s_{1}\right) \\ V\left(s_{2}\right) \\ \vdots \\ V\left(s_{N}\right)\end{array}\right]$

​                                                                                               $V=R+\gamma P V$

​                                                                                               $V=(I-\gamma P)^{-1} R$

在求解的过程中有两个比较重要的问题：

- 怎么保证矩阵是可逆的？
- 矩阵求逆的复杂度是$O\left(N^{3}\right)$，所以这种求解方式只对小的MRP的问题有效

##### Q（s,a)

动作价值函数，我们一般也会直接称q函数定义了状态-动作对的好坏，其实我们只要知道了V函数，Q函数包括我们后面讲的A（优势）函数都可以求出来，而如果知道这三个函数中的任何一个，整个MDP其实就是可解的。但是我们没办法一开始就知道，所以我们一般都会通过一个初始化的价值函数或者直接初始化一个策略去采样去然后去估算价值函数，然后再根据新的价值函数再去采样，这样理想情况下我们对价值函数的估计越来越准，决策也越来越好。

The action-value function $q^{\pi}(s, a)$ is the expected return starting from state $s,$ taking action $a,$ and then following policy $\pi$

​                                                                                      $q^{\pi}(s, a)=\mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s, A_{t}=a\right]$

​                                                                                        $v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s) q^{\pi}(s, a)$

##### Bellman Expection Eqution

$v^{\pi}(s)=E_{\pi}\left[R_{t+1}+\gamma v^{\pi}\left(s_{t+1}\right) \mid s_{t}=s\right]$

$q^{\pi}(s, a)=E_{\pi}\left[R_{t+1}+\gamma q^{\pi}\left(s_{t+1}, A_{t+1}\right) \mid s_{t}=s, A_{t}=a\right]$
$$
\begin{aligned}
v^{\pi}(s) &=\sum_{a \in A} \pi(a \mid s) q^{\pi}(s, a) \\
q^{\pi}(s, a) &=R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi}\left(s^{\prime}\right)
\end{aligned}
$$
Thus，将两个方程互相带入彼此
$$
v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi}\left(s^{\prime}\right)\right)
$$

$$
q^{\pi}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) q^{\pi}\left(s^{\prime}, a^{\prime}\right)
$$

以上等式表征了当前的Q函数和V函数与将来的Q函数和V函数之间的关系

其实就是下面的两个图：
![image-20200827201233326](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200827201233326.png)

![image-20200827201253574](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200827201253574.png)

##### Bellman Optimality Equation

$$
\begin{aligned}
v^{*}(s) &=\max _{a} q^{*}(s, a) \\
q^{*}(s, a) &=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{*}\left(s^{\prime}\right)
\end{aligned}
$$

thus
$$
\begin{aligned}
v^{*}(s) &=\max _{a} R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{*}\left(s^{\prime}\right) \\
q^{*}(s, a) &=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \max _{a^{\prime}} q^{*}\left(s^{\prime}, a^{\prime}\right)
\end{aligned}
$$
​                                                                                                 $$\begin{aligned}
v_{*}(s) &=\max _{a \in \mathcal{A}(s)} q_{\pi_{*}}(s, a) \\
&=\max _{a} \mathbb{E}_{\pi_{*}}\left[G_{t} \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{*}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a \in \mathcal{A}(s)} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{*}\left(s^{\prime}\right)\right]
\end{aligned}$$
最后两个方程是 $v_{*}$ 的贝尔曼最优方程的两种形式, $q_{*}$ 的贝尔曼最优方程为
$$
\begin{aligned}
q_{*}(s, a) &=\mathbb{E}\left[R_{t+1}+\gamma \sum_{a^{\prime}} q_{*}\left(S_{t+1, a^{\prime}}\right) \mid S_{t}=s, A_{t}=a\right] \\
&=\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \sum_{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)\right]
\end{aligned}
$$
贝尔曼最优方程其实阐述了一个事实：**最优策略下各个状态的价值一定等于这个状态下最优动作的期望回报。**

对于有限的MDP， $v_{*}$ 的贝尔曼最优方程具有唯一解。贝尔曼最优方程实际上是一个方程组，每个状态一个方程，所以如果有 $n$ 个状态, 则有 $n$ 个未知数的 $n$ 个方程。 如果环境的动态 $p$ 是已知，则原则上可以使用解决非线性方程组的各种方法中的任何一种来求解该 $v_{*}$ 的方程组。 同样, 可以求解 $q_{*}$ 的一組相关方程。

一旦有 $v_{*},$ 确定最优策略就比较容易了。对于每个状态, 将在贝尔曼最优方程中获得最大价值的一 个或多个动作。 任何仅为这些操作分配非零概率的策略都是最优策略。你可以将其视为一步步的搜索。 **如果具有最优价值函数 $v_{*}$, 则在一步搜索之后出现的动作将是最优动作**。 **另一种说法的方法是任何对最优评估函数 $v_{*}$ 贪婪的策略是最优策略**。

计算机科学中使用术语贪楚来描述任何基于本地或直接考虑来选择替代搜索或决策程序的方法，而不考虑这种选择可能阻止未来获得更好的替代方法的可能性。因此，它描述了根据其短期结果选择行动的策略。

 $v_{*}$ 的美妙之处在于，如果用它来评估行动的短期结果，具体来说是一步到位的结果， 那么贪婪的策略在我们感兴趣的长期意义上 实际上是最优的，因为 $v_{*}$ 已经考虑到所有可能的未来动作的奖励结果。**定义 $v_{*}$的意义就在于，我们可以将最优的长期（全局）回报期望值转化为每个状态对应的一个当前局部量的计算**。因此, 一步一步的搜索产生长期的最佳动作。

有 $q_{*}$ 使选择最优动作更容易。 给定$q_{*},$ agent基至不需要进行单步搜索的过程， 对于任何状态 $s$ 它可以简单地发现任何使 $q_{*}(s, a)$ 最大化的动作。 $\quad$动作价值函数有效地缓存了所有单步搜索的结果。 它将最优的长期回报的期望作为本地并立即可用于每个状态一动作对的值。因此, 代表状 态-动作对的功能而不仅仅是状态的代价， 最优动作-价值函数允许选择最优动作而不必知道关于可能的后继状态及其值的任何信息，即不需要知道任何环境的动态变化特性了。

总结也就是说有了$v_{*}$之后，还需要做单步搜索，但是有了$q_{*}$后，就不需要了。

显示地求解贝尔曼最优方程提供了求解最优策略的一条途径，从而为解决强化学习问题提供了依据。 但是, 这个解决方案很少直接有用。它类似于穷举搜索，展望所有可能性,计算每种可能性出现的概率及其期望收益。这个解决方案依赖于至少三个假设，在实践中很少是这样的:

- **我们准确地知道环境的动态特性**；
- **我们有足够的计算资源来完成解决方案的计算;**
- **马尔可夫性。**

 对于我们感兴趣的任务，通常不能完全满足这三个条件。 例如，虽然第一和第三个假设对于西洋双陆棋游戏没有任何问题, 但第二个是主要的障碍。 因为游戏有 $10^{20}$ 个状态, 所以今天最快的电脑需要数千年的时间才能计算出 $v_{*}$ 的贝尔曼方程式, 而找到 $q_{*}$ 也是如此。在强化学习中，我们通常只能用近似解法来解决这类问题。