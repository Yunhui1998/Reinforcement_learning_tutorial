## 第九章-单调提升的策略优化

#### Problems in Policy Gradient

到这里为止, policy gradient中一直存在两个比较重要的问题。

第一个问题是sample efficiency的问题：由于算法是on-policy的，每次进行完梯度上升后都将数据丢弃。而我们引入了importance sampling进行off-policy的policy gradient, 这样存在的问题则是稳定性降低了，推导式中存在两个 policy连乘的比值, **比较容易出现消失或者爆炸的现象**:
$$
\begin{aligned}
\nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right) &=E_{\tau \sim \pi_{\theta}(\tau)}\left[\frac{\pi_{\theta^{\prime}}(\tau)}{\pi_{\theta}(\tau)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\tau) r(\tau)\right] \quad \text { when } \theta \neq \theta^{\prime} \\
&=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\prod_{t=1}^{T} \frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\right)\left(\sum_{t=1}^{T} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right]
\end{aligned}
$$
后续将从理论层面讨论两个policy之前差异带来的影响。

第二个问题则是来自gradient ascent, 由于策略优化是在parameter space上做的更新, 但是其实 parameter space并不等价于policy space, 可以看到如下的一个例子:

![preview](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/v2-07f089c50ed3899bcf0fe6dadefbc7f4_r.jpg)

参数空间上一个小的变化在策略（动作空间）上带来了巨大的差异。

上面两个问题, 也就给算法提出了更多的要求：

- 如何在引入importance sampling的情况下尽可能避免policy差异过大的问题。
-  如何在保证policy不发生突变的情况下进行参数的更新。

#### Issues of Importance Sampling

我们在off-policy的方法中经常会用到importance sampling的方法，那么我们用一个分布去对另一个分布的期望进行求解的这种方法有什么问题呢？---**问题就在分布的方差上面：**

我们知道：

​                                                                                                  $$\begin{array}{l}\operatorname{VAR}[X] =E\left[X^{2}\right]-(E[X])^{2}\end{array}$$

而在重要性采样中，期望和原来的期望没有差别：

​                                                                                                 $$E_{x \sim p}[f(x)]=E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]$$

那我们来看一下importance sampling的方差和原来的方差有什么差异：

​                                                                                                 $$\begin{array}{l}
\operatorname{Var}_{x \sim p}[f(x)]=E_{x \sim p}\left[f(x)^{2}\right]-\left(E_{x \sim p}[f(x)]\right)^{2} \\
\begin{aligned}
\operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right] &=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2} \\
&=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)}-\left(E_{x \sim p}[f(x)]\right)^{2}\right.
\end{aligned}
\end{array}$$

我们发现上下两项唯一的差别就在$$\frac{p(x)}{q(x)}$$，也就是说当$$\frac{p(x)}{q(x)}$$值很大或者很小的时候，方差就会差的比较多。

我们可以看下面的图：

当p(x)和q(x)差的比较多的时候，如果我们在左边对f(x)进行采样，那么最终得到的期望就是负的；如果在右边对f(x)进行采样，那么最终得到的期望就是正的，说明了当p(x)和q(x)差的比较多的时候，很容易出现方差过大的问题。

当然如果采样的数量足够多，即使我们采样的大部分的点都在右边，但是之后采到了左边一些数据，因为$$\frac{p(x)}{q(x)}$$比值比较大，那么这个权重就会把左边的这个负值放大，**所以其实如果采样的数量足够多，这个方差的问题还是不会存在，但是事实是我们现实中采样的数量都是有限的**。

![image-20210308113041201](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308113041201.png)

#### Monotonic Improvement Theory（策略的单调提升）

在这一部分，我们希望能找到一种方法，使得策略模型在优化的过程中单调提升，要做到这一点，我们首先要解决的一个问题就是怎么衡量两个策略（更新前和更新后的策略）之间的差异：

​                                                                                          $$\begin{aligned}
J\left(\theta^{\prime}\right)-J(\theta) &=J\left(\theta^{\prime}\right)-E_{\mathbf{s}_{0} \sim p\left(\mathbf{s}_{0}\right)}\left[V^{\pi_{\theta}}\left(\mathbf{s}_{0}\right)\right] \\
&=J\left(\theta^{\prime}\right)-E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[V^{\pi_{\theta}}\left(\mathbf{s}_{0}\right)\right] \\
&=J\left(\theta^{\prime}\right)-E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)-\sum_{t=1}^{\infty} \gamma^{t} V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right] \\
&=J\left(\theta^{\prime}\right)+E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right] \\
&=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]+E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right] \\
&=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right] \\
&=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
\end{aligned}$$

在这里我们稍微介绍一些上面的证明做了：

- 第一行：将old policy展开为对它生成的trajectory的期望。
- 第二行：由于两个策略关于价值函数的expectation是相同的，所以可以将对old policy的期望转化为new policy的期望，这是这里转化的最关键一步。
- 在三到四步，做了一个简单的构造，
- 在第五步将new policy拆分，由于两个部分都转化为关于new policy的expectation，所以就可以将它们合并，则可以得到最终的关系式。

也就是说，**我们的新策略与旧策略的差就是旧策略的advantage关于新策略的trajectory的expectation**。这个时候，如果按照policy iteration的流程，在improvement中，也就只需要使得每步提升最大，找到新的parameter使得下面的这个式子最大化即可。

​                                                                                     $$J\left(\theta^{\prime}\right)-J(\theta)=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$$

然后我们对后面一项进行重要性采样：

​                                                                                   $$\begin{aligned}
E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] &=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right] \\
&=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]
\end{aligned}$$



我们发现前面一项还是存在新策略的expectation的问题，使得这个问题就变得异常复杂，那么我们能不能在这里直接把新策略替换成旧策略呢？这样我们就可以根据旧策略去算这个值了：

![image-20210308115646170](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308115646170.png)

这里我们有一个假设或者可以说是一个先验，**也就是当我们的新策略与旧策略比较接近时，我们得到的$$p_{\theta}\left(\mathbf{s}_{t}\right)$$和$$p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)$$就会比较接近，那么我们就可以对前面两项期望做替换。**

那么我们现在就来证明它：

- 首先证明确定性策略的情况下：


![image-20210308132638838](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308132638838.png)

- 接着证明策略是任意分布下的情况：


![image-20210308132721992](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308132721992.png)

​        最终也就得到了一个bound，只要两个policy在policy space足够相近，那么就可以直接对expectation进行替换，进行policy improvement，同时能够有收敛保证：
$$
\theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]
$$
​       such that $\left|\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right| \leq \epsilon$，  for small enough $\epsilon,$ this is guaranteed to improve $J\left(\theta^{\prime}\right)-J(\theta)$

#### Monotonic Improvement with KL Divergence

上面直接关于policy probability做差取绝对值的约束实际上是比较难优化的, 所以我们希望能够找个一个更容易优化的约束函数, 从而降低求解难度。所以在这里我们引入KL散度：

关于两个policy distribution的KL divergence可以由如下式子定义:
$$
D_{\mathrm{KL}}\left(p_{1}(x) \| p_{2}(x)\right)=E_{x \sim p_{1}(x)}\left[\log \frac{p_{1}(x)}{p_{2}(x)}\right]
$$
**KL散度表征的是策略之间的差异。**

从它的性质也可以推出如下的性质:
$$
\left|\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right| \leq \sqrt{\frac{1}{2} D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) \| \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)}
$$
也就是说**如果我们将KL divergence约束住了，那么原始约束函数也就满足了，两者存在转化的等价性**。所以优化的目标也就转化为如下的形式, 约束发生了改变, 问题更容易优化了。

![image-20210308161628830](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308161628830.png)

在上面将整个问题转化为对KL divergence约束的优化问题，那么在这里就将讨论各种求解这个优化问题的方式。根据不同的近似方法，就可以得到不同的算法。

![image-20210308161957370](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210308161957370.png)

#### 共轭梯度法实现策略上的单调提升(Monotonic Improvement with  Dual gradient descent)

我们先介绍一下共轭梯度法，再介绍一下共轭梯度法在策略的单调提升上的应用。

首先，开门见山地介绍一些共轭梯度法：**共轭梯度法的目的：让每一次优化变得“极致”**。

前面我们一般都用策略梯度法去解决优化问题。虽然梯度下降法的每一步都朝着局部最优的方向前进，但它在不同的迭代轮数会选择非常相近的方向，这说明当某一次选择了一个更新方向和步长后，这个方向并没有被更新完，未来还会存在这个方向的残差。如果把参数更新的轨迹显示出来，我们可以看到有时轨迹会走成 下面这种zig-zag 的形状：

![image-20210309110544217](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210309110544217.png)

于是，我们对优化提出了更高的要求：**每朝一个方向走，就把这个方向优化到极致，使得之后的更新再也不需要朝这个方向进行**。我们引入一个变量一一误差，假设最优的参数为$x^{*},$ 当前第 $t$ 轮的参数为 $x_{t},$ 误差可以定义为
$$
e_{t}=x^{*}-x_{t}
$$
这个**误差表示了参数的最优点和当前点之间的距离**。那么目标就更明确了，我们**希望每一步优化后，当前的误差和刚才优化的方向正交**。现在令 $r_{t}$ 表示第 $t$ 轮的更新方向，就可以得到下面的公式：

​                                                                                                                              $r_{t}^{\mathrm{T}} e_{t+1}=0$
**假设每一轮的优化量都和误差正交，那么如果我们的优化空间有 $d$ 维，理论上最多只需要迭代 $d$ 轮就可以求解出来，这样在优化时间上就有了保证**。如果我们直接使用这个公式，就会发现一个问题: 公式中需要知道误差。换句话说，我们需要知道最优点在哪儿。如果我们知道最优点在哪儿，就不用优化了。 可是不知道最优点儿，这个方法又无法直接使用，仿佛陷入了一个死循环，那么接下来我们就利用数学工具来试着推导解决这个问题。

前面我们提到优化方向和误差正交，如果使用了共轭这个工具，现在两者的关系将变为共轭正交，也就是存在一个矩阵 $A(A$就是轭 )，使得优化方向和误差满足正交的性质:
$$
r_{t}^{\mathrm{T}} A e_{t+1}=0
$$
等等，这不就说明轭的存在使原本正交的两项变得不正交了吗? 但其实如果我们在上面的原始公式中间加一个特殊的矩阵，例如单位阵：
$$
r_{t}^{\mathrm{T}} \boldsymbol{I} \boldsymbol{e}_{t+1}=0
$$
加入单位阵并不会改变结果，所以原本正交的二者依然正交。所以换个角度理解，**我们以前简单的正交都是在单位阵这个简单且性质优良的“轭”的下实现的**。所以回到正交中，如果使用单位阵作为轭, 那么绑定在一起的就是常规意义上正交的一对向量; 如果使用其他矩阵，那么共轭正交的向量肯定也会满足其他的性质。

明确了共轭梯度法的目标和特点，我们就要开始推导算法公式了。共轭梯度法属于线搜索的一种，因此和梯度下降法类似，我们的总体思路不变，优化过程分如下两步:

- 确定优化方向

- 确定优化步长。

我们先来介绍优化步长的计算方法。假设当前的参数为 $X_{t},$ 我们已经得到了优化方向 $r_{t},$ 下面要确定的就是步长 $\alpha_{t},$ 根据前面提过的共轭正交公式
$$
\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{e}_{t+1}=0
$$
我们可以开始推导:
$$
\begin{aligned}
r_{t}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{e}_{t+1} &=\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A}\left[\boldsymbol{e}_{t}+\boldsymbol{X}_{t}-\boldsymbol{X}_{t+1}\right] \\
&=\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A}\left[\boldsymbol{e}_{t}+\alpha_{t} \boldsymbol{r}_{t}\right] \\
&=\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{e}_{t}+\alpha_{t} \boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{r}_{t}=0
\end{aligned}
$$
于是可以整理得到
$$
\begin{array}{l}
\alpha_{t}=-\frac{r_{t}^{\mathrm{T}} A e_{t}}{r_{t}^{\mathrm{T}} A r_{t}} \\
\alpha_{t}=-\frac{r_{t}^{\mathrm{T}} A\left(\boldsymbol{X}^{*}-\boldsymbol{X}_{t}\right)}{\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A} r_{t}}
\end{array}
$$
我们知道 $A X^{*}=b,$ 第 $t$ 轮的梯度 $g_{t}=A X_{t}-b,$ 于是公式最终变为
$$
\alpha_{t}=\frac{\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{g}_{t}}{\boldsymbol{r}_{t}^{\mathrm{T}} \boldsymbol{A} r_{t}}
$$
**到这里，我们利用矩阵 $A$ 成功地把公式中的误差 $e$ 抵消，于是步长变得可解**。

完成了步长的求解，接下来就要回到第一步看看优化方向的计算方法。我们要解决的主要问题是如何让优化方向和误差正交。由于每一次的优化后，剩下的误差和本次的优化正交（共轭正交 ) , 所以可以看出每一个优化方向彼此间都是正交的。那么，我们接下来就用Gram-Schmidt 方法来构建这些彼此正交的向量。

在线性代数课上, 我们曾经学过一个向量正交化的方法一 Gram-Schmidt 方法。这个算法的输入是 $N$ 维空间中 $N$ 个线性无关的向量，由于向量间线性无关，任何一个向量都无法通过其他向量表达出来。算法的输出是 $N$ 个相互正交的向量，也就是我们最终想要的向量组合。它的具体算法如下：

令输入向量为 $u_{1}, u_{2}, \cdots, u_{N},$ 输出向量为 $d_{1}, d_{2}, \cdots, d_{N},$ 那 么有:

-  对于第一个向量，我们保持它不变: $u_{1}=d_{1} $

- 对于第二个向量，我们去掉其中和第一个向量共线的部分，令去掉的比例为 $\beta_{i},$ 所以第二个向量 $d_{2}$ 等于 $u_{2}+\beta_{1} d_{1 }$

- 对于第三个向量，我们去掉其中和第一、第二个向量共线的部分: $d_{3}=\boldsymbol{u}_{3}+$ $\sum_{i=1}^{2} \beta_{3, i} d_{i }$

- 对于第 $N$ 个向量，我们去掉其中和第一、第二、第 $N-1$ 个向量共线的部分 :$\boldsymbol{d}_{N}=\boldsymbol{u}_{N}+\sum_{i=1}^{N-1} \beta_{N, i} \boldsymbol{d}_{i }$

  那么我们怎么求这些比例项 $\beta$ 呢? 我们利用前面提到的性质，向量之间正交（这里还是共轭正交 )，于是有

$$
d_{l}^{\mathrm{T}} A d_{t}=0,(l=1,2, \ldots, t-1)
$$

​       进一步展开，可以得到：
$$
\boldsymbol{d}_{l}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{d}_{t}=\boldsymbol{d}_{l}^{\mathrm{T}} \boldsymbol{A}\left(\boldsymbol{u}_{t}+\sum_{i=1}^{t-1} \beta_{t, i} \boldsymbol{d}_{i}\right)=\boldsymbol{d}_{l}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{u}_{t}+\boldsymbol{d}_{l}^{\mathrm{T}} \boldsymbol{A} \sum_{i=1}^{t-1} \beta_{t, i} \boldsymbol{d}_{i}
$$



​     利用正交的性质，可以将公式化简为
$$
\boldsymbol{d}_{l}^{\mathrm{T}} \boldsymbol{A} u_{t}+\boldsymbol{d}_{l}^{\mathrm{T}} \boldsymbol{A} \beta_{t, l} \boldsymbol{d}_{l}=0
$$
​     所以最终我们得到
$$
\beta_{t, l}=-\frac{d_{l}^{\mathrm{T}} A u_{l}}{d_{l}^{\mathrm{T}} \boldsymbol{A} d_{l}}
$$
求得了这个变量, 就可以依此求出当前向量与其他向量的共线部分, 从而确保所有向量相互正交的性质。当然，我们也看到，**为了实现正交的效果，算法的复杂度为 $O\left(N^{2}\right),$ 在实现速度上不是很快**。

接下来我们需要解决如下两个问题：

-  我们要用什么向量构建这些正交向量?
- 随着向量数量的增加，我们需要计算的参数越来越多，我们一共要计算 $O\left(N^{2}\right)$ 数量级的 $\beta,$ 这个计算的数目还是有点多，能不能再减少呢?

要解决上面两个问题我们需要先证明下面三个推论：

1. 第 $t$ 步计算的梯度 $g_{t}$ 和前 $t-1$ 步的更新 $\left\{d_{j}\right\}_{j=1}^{t-1}$ 正交
2. 第 $t$ 步计算的梯度 $g_{t}$ 和前 $t-1$ 步的梯度 $\left\{\boldsymbol{g}_{t}\right\}_{j=1}^{t-1}$ 正交
3. 第 $t$ 步计算的梯度 $g_{t}$ 和前 $t-2$ 步的更新 $\left\{\boldsymbol{d}_{j}\right\}_{j=1}^{t-2}$ 共轭正交。

先来证明第一个推论：前面提到共轭梯度法的基本思想：每一轮把某一方向优化彻底，保证后面的优化不再对这个方向做任何操作。假设算法一共进行了 $T$ 轮迭代，我们就可以用这 $T$ 轮求出的优化方向组合成最终的误差。假设我们的初始参数值为 $x_{1},$ 那么它到最优点 $x^{*}$ 的距离为 $e_{1},$ 这也是当前的误差。第 $t$ 步求出的优化方向为 $d_{t},$ 每一轮的优化步长为 $\gamma_{i},$ 根据上面的定义，可以得到如下公式：
$$
e_{1}=\sum_{i=1}^{T} \gamma_{i} d_{i}
$$
这样我们就用更新方向表示了误差。对于不同迭代轮数的误差，我们也可以用类似的公式表示。上面的公式利用所有的更新组合成误差，我们反过来思考也可以理解，通过每一步的更新，算法最终收敛，误差变为 $0 。$ 所以，这里的 $\gamma$ 和前面提到的步长 $\alpha$之间是等价的关系。
下面就来证明 :

$$\text { 当 } i<j \text { 时， }$$
$$
\begin{array}{l}
\qquad \boldsymbol{d}_{i}^{\mathrm{T}} \boldsymbol{g}_{j}=0
\end{array}
$$
证明过程如下：

​                                                                                                                             $$\boldsymbol{d}_{i}^{\mathrm{T}} \boldsymbol{g}_{j}=\boldsymbol{d}_{i}^{\mathrm{T}}\left(\boldsymbol{A} \boldsymbol{X}_{j}\right)$$

​                                                                                                                             $$\begin{array}{l}
=d_{i}^{\mathrm{T}}\left(\boldsymbol{A} \boldsymbol{X}_{j}-\boldsymbol{A} \boldsymbol{X}^{*}\right) \\
=d_{i}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{e}_{j} \\
=d_{i}^{\mathrm{T}} \boldsymbol{A}\left(\sum_{t=j}^{T} \gamma_{t} \boldsymbol{d}_{t}\right)
\end{array}$$

因为前面已经说明任意两个优化方向是相互正交的 ( Gram-Schmidt 方法 ), 所以这个式子的结果为 0 。利用共轭正交的性质，我们得到了更好的正交性质，这就是共轭梯度法最精髓的地方下面证明第二个推论。我们直接给出推导过程：
$$
\boldsymbol{g}_{i}^{\mathrm{T}} \boldsymbol{g}_{j}=\left(\boldsymbol{d}_{i}+\sum_{j=1}^{i-1} \beta_{j} \boldsymbol{d}_{j}\right)^{\mathrm{T}} \boldsymbol{g}_{j}=0
$$
此时我们发现，由于每一步的梯度与此前的梯度正交，因此我们可以使用它作为线性无关的向量组，从而组成共轭正交的向量组。第三个推论将以第二个推论为基础，也就是
$$
\boldsymbol{g}_{i}^{\mathrm{T}} \boldsymbol{g}_{j}=0(i<j)
$$
将公式进行变换，可以得到
$$
\begin{aligned}
\boldsymbol{g}_{j+1}^{\mathrm{T}} \boldsymbol{g}_{i} &=\left(\boldsymbol{A} \boldsymbol{X}_{j+1}\right)^{\mathrm{T}} \boldsymbol{g}_{i} \\
&=\left(\boldsymbol{A}\left(\boldsymbol{X}_{j}+\alpha_{j} \boldsymbol{d}_{j}\right)\right)^{\mathrm{T}} \boldsymbol{g}_{i} \\
&=\left(\boldsymbol{A} \boldsymbol{X}_{j}\right)^{\mathrm{T}} \boldsymbol{g}_{i}+\left(\boldsymbol{A} \alpha_{j} \boldsymbol{d}_{j}\right)^{\mathrm{T}} \boldsymbol{g}_{i}
\end{aligned}
$$
​                                                                                                                      $=g_{j}^{\mathrm{T}} \boldsymbol{g}_{i}+\alpha_{j} \boldsymbol{d}_{j}^{\mathrm{T}} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{g}_{i} \quad(\boldsymbol{A}$ 是对称矩阵 $)$
$$
=g_{j}^{\mathrm{T}} g_{i}+\alpha_{j} d_{j}^{\mathrm{T}} A g_{i}
$$
对公式进行变换，可以得到
$$
\boldsymbol{d}_{j}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{g}_{i}=\frac{1}{\alpha_{j}}\left[\boldsymbol{g}_{j+1}^{\mathrm{T}} \boldsymbol{g}_{i}-\boldsymbol{g}_{j}^{\mathrm{T}} \boldsymbol{g}_{i}\right]
$$
此时我们发现公式的左边就是求解 $\beta$ 的分子，那么公式右边等于什么呢? 我们令 $j<i,$于是有:

- 当 $j=i-1$ 时，等式右边为 $\frac{1}{\alpha_{j}}\left[\boldsymbol{g}_{i}^{\mathrm{T}} \boldsymbol{g}_{i}-\boldsymbol{g}_{i-1}^{\mathrm{T}} \boldsymbol{g}_{i}\right],$ 可以看出当 $g_{i}$ 不等于 0 时，这部分公式不等于 0 
- 当 $j<i-1$ 时，等式右边为 $\frac{1}{\alpha_{j}}\left[\boldsymbol{g}_{j+1}^{\mathrm{T}} \boldsymbol{g}_{i}-\boldsymbol{g}_{j}^{\mathrm{T}} \boldsymbol{g}_{i}\right],$ 由于 $j+1<i, j<i,$ 根据前面的推断，这部分公式为 $0_{}$
  所以可以得到，使用 Gram-Schmidt 方法可以保证更新方向共轭正交。对于第 $t$ 轮优化，我们只计算 $\beta_{t-1}$ 即可，其他的 $\beta$ 值均为 $0,$ 不用计算，这样计算量得到了极大的降低。

到这里我们完成了共轭梯度法的理论推导，由于算法比较复杂，我们需要对算法进行回顾：

- 最初，我们希望我们的更新方向与误差正交，从而得出了 $d_{i}^{\mathrm{T}} e_{i}=0$ 。
- 由于误差无法提前知道，直接计算正交比较困难，所以我们将正交改为共轭正交，于是公式变为 $d_{i}^{\mathrm{T}} A e_{i}=0_{\circ}$
-  我们希望使用函数的梯度作为优化方向的基础，每一次求出当前的梯度，就用梯度减去之前更新方向的成分，以保证每一次的优化都保持共轭正交。
- 我们采用 Gram-Schmidt 方法确保更新方向能够维持共轭正交的性质，根据证明，我们发现在使用梯度时，每一轮只需要计算一个 $\beta$ 值就能实现共轭正交的效果，计算量大大降低。 $\beta$ 的求解方法为 $\beta_{t, l}=-\frac{d_{1}^{\mathrm{T}} A u_{l}}{d_{l}^{T} A d_{l}}$ 。
- 完成了更新方向的计算，我们根据前面的推论，计算每一步的步长: $\alpha_{t}=$ $\frac{r_{\mathrm{t}}^{\mathrm{T}} g_{t}}{r_{t}^{\mathrm{T}} A r_{t}}$

然后我们在这里放一下共轭梯度的实现代码，代码来自于Baselines项目：

```python
import numpy as np


def conjugate_gradient(f_ax, b_vec, cg_iters=10, callback=None, verbose=False, residual_tol=1e-10):
    """
    conjugate gradient calculation (Ax = b), bases on
    https://epubs.siam.org/doi/book/10.1137/1.9781611971446 Demmel p 312

    :param f_ax: (function) The function describing the Matrix A dot the vector x
                 (x being the input parameter of the function)
    :param b_vec: (numpy float) vector b, where Ax = b
    :param cg_iters: (int) the maximum number of iterations for converging
    :param callback: (function) callback the values of x while converging
    :param verbose: (bool) print extra information
    :param residual_tol: (float) the break point if the residual is below this value
    :return: (numpy float) vector x, where Ax = b
    """
    first_basis_vect = b_vec.copy()  # the first basis vector
    residual = b_vec.copy()  # the residual
    x_var = np.zeros_like(b_vec)  # vector x, where Ax = b
    residual_dot_residual = residual.dot(residual)  # L2 norm of the residual

    fmt_str = "%10i %10.3g %10.3g"
    title_str = "%10s %10s %10s"
    if verbose:
        print(title_str % ("iter", "residual norm", "soln norm"))

    for i in range(cg_iters):
        if callback is not None:
            callback(x_var)
        if verbose:
            print(fmt_str % (i, residual_dot_residual, np.linalg.norm(x_var)))
        z_var = f_ax(first_basis_vect)
        v_var = residual_dot_residual / first_basis_vect.dot(z_var)
        x_var += v_var * first_basis_vect
        residual -= v_var * z_var
        new_residual_dot_residual = residual.dot(residual)
        mu_val = new_residual_dot_residual / residual_dot_residual
        first_basis_vect = residual + mu_val * first_basis_vect

        residual_dot_residual = new_residual_dot_residual
        if residual_dot_residual < residual_tol:
            break

    if callback is not None:
        callback(x_var)
    if verbose:
        print(fmt_str % (i + 1, residual_dot_residual, np.linalg.norm(x_var)))
    return x_var

```

关于共轭梯度法的介绍到这就告一段落了，接下来我们回顾一下我们前面策略提升的问题：

![image-20210319231059980](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210319231059980.png)

针对这个问题，我们利用共轭梯度法将将优化目标中的约束条件转化为优化目标的一部分，从而将对目标函数在约束下进行优化的过程等价为直接优化对偶目标函数的行为。原始的优化目标在引入拉格朗日乘子后可以转化为如下的形式：

​                                         $$\mathcal{L}\left(\theta^{\prime}, \lambda\right)=\sum_{t} E_{s_{t} \sim p_{o}\left(s_{t}\right)}\left[E_{a_{t} \sim \pi_{\theta}\left(a_{t}, \mid s_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]-\lambda\left(D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) \| \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)-\epsilon\right)$$

进而可以进行迭代求解：

![image-20210319231319794](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210319231319794.png)

从直观上理解，将约束引入目标函数后, 乘子 $\lambda$ 也是一个需要优化的参数。当约束违反程度大的时候, 后面拉格朗日项就变成特别大的负项，如果需要最大化这个目标，则需要将乘子变大，反之依然。通过对这个乘子的调节，进而修正约束部分的重要性，从而达到自适应优化的目标。

#### 自然梯度法实现策略上的单调提升(Monotonic Improvement  with Natural gradient descent)

 和上一篇一样，我们先介绍一下自然梯度法，再介绍一下自然梯度法在策略上的单调提升的应用。

我们在使用梯度下降时，当优化问题的两个坐标轴的尺度差异比较大时，使用统一的学习率会产生问题：某一个坐标轴可能发散。这是因为，优化问题中参数尺度一般都是不同的。因此，在不同的优化曲面上, 虽然每一轮迭代对参数的更新量相差不多，但是它们对模型的影响完全不同，对不同参数进行同样数值大小的更新，不一定带来同样的模型改变，反过来也是如此。**而梯度下降法不简单地使用学习率对参数更新进行量化，而是对模型效果进行量化**。

自然梯度法的具体形式如下：

​                                                                                                $$\begin{array}{l}
\operatorname{minimize}_{\Delta w} f(w)+\nabla_{w} f(w) \Delta w \\
\text { s.t. } \operatorname{KL}(f(w), f(w+\Delta w))<\epsilon
\end{array}$$

可以看出，有了模型层面的约束，每一轮迭代无论参数发生多大的变化，模型的变化都会限制在一定的范围内，因此不论我们使用什么样的模型， 这个约束都会起到相同的效果，因此这个约束是具有普适性的，在任何模型上都能发挥同样稳定的效果。但是问题在于，我们怎么求解更新前和更新后的策略的KL散度？

下面我们就通过Fisher信息矩阵来推导一下：
$$
\begin{array}{l}
\operatorname{KL}(f(w) \| f(w+\Delta w)) \\
=E_{f_{w}}\left[\log \frac{f(w)}{f(w+\Delta w)}\right] \\
=E_{f_{w}}[\log f(w)]-E_{f_{w}}[\log f(w+\Delta w)]
\end{array}
$$
其中 $E$ 表示为依 $f(w)$ 概率计算得到的期望。对等式右边的第二项进行二阶泰勒展开，可以得到
$$
\begin{array}{l}
\approx E_{f_{w}}[\log f(w)]-E_{f_{w}}\left[\log f(w)+\nabla_{w} \log f(w) \Delta w+\frac{1}{2} \Delta w^{\mathrm{T}} \nabla_{w}^{2} \log f(w) \Delta w\right] \\
=E_{f_{w}}[\log f(w)]-E_{f_{w}}[\log f(w)]-E_{f_{w}}\left[\nabla_{w} \log f(w) \Delta w\right]-E_{f_{w}}\left[\frac{1}{2} \Delta w^{\mathrm{T}} \nabla_{w}^{2} \log f(w) \Delta w\right] \\
=-E_{f_{w}}\left[\nabla_{w} \log f(w) \Delta w\right]-E_{f_{w}}\left[\frac{1}{2} \Delta w^{\mathrm{T}} \nabla_{w}^{2} \log f(w) \Delta w\right]
\end{array}
$$
​                                                          $$\begin{array}{l}
=-\int_{x} f(w) \nabla_{w} \log f(w) \Delta w \mathrm{~d} x-\int_{x} f(w) \frac{1}{2} \Delta w^{\mathrm{T}} \nabla_{w}^{2} \log f(w) \Delta w \mathrm{~d} x \\
=-\left[\int_{x} f(w) \frac{1}{f(w)} \nabla_{w} f(w) \mathrm{d} x\right] \Delta w-\frac{1}{2} \Delta w^{\mathrm{T}}\left[\int_{x} f(w) \nabla_{w}^{2} \log f(w) \mathrm{d} x\right] \Delta w \\
=-\left[\int_{x} \nabla_{w} f(w) \mathrm{d} x\right] \Delta w-\frac{1}{2} \Delta w^{\mathrm{T}} E_{f_{w}}\left[\nabla_{w}^{2} \log f(w)\right] \Delta w
\end{array}$$

由于我们定义的函数 $f(w)$ 一般都是一个连续、可导、有界、性质优良的函数,所以这 里第一项的积分和微分可以互换，同时我们将简写的 $f(w)$ 用完整的形式写出: $f(x ; w),$
于是上式的第一项就变为
$$
\begin{array}{l}
=-\left[\nabla_{w} \int_{x} f(x ; w) \mathrm{d} x\right] \Delta w \\
=-\left[\nabla_{w} 1\right] \Delta w \\
=0
\end{array}
$$
最终得到
$$
\mathrm{KL}(f(w) \| f(w+\Delta w))=-\frac{1}{2} \Delta w^{\mathrm{T}} E_{f_{w}}\left[\nabla_{w}^{2} \log f(w)\right] \Delta w
$$
这里包含一个二阶导的期望值，虽然看上去比 KL 散度直观，但它仍然比较复杂。
我们需要用 Fisher 信息矩阵（Fisher Information Matrix ) 来表示它。Fisher 信息是信息几何中的一个概念，它也被应用到机器学习中。前面提到 $f(w)$ 表示某个概率分布，我们首先定义 Score 函数（ Score Function ) 为对数似然函数的一阶导数
$$
l_{f_{w}}=\nabla_{w} \log f(x ; w)
$$
通过计算可以发现，score 函数的期望值为 0，公式推导如下:
$$
\begin{aligned}
E_{f(w)}\left[l_{f_{w}}\right] &=\int_{x} f(w) \nabla_{w} \log f(w) \mathrm{d} x \\
&=\int_{x} f(w) \frac{\nabla_{w} f(w)}{f(w)} \mathrm{d} x \\
&=\int \nabla_{w} f(w) \mathrm{d} x \\
&=\nabla_{w} \int_{x} f(x ; w) \mathrm{d} x \\
&=\nabla_{w} 1 \\
&=0
\end{aligned}
$$
Fisher 信息矩阵可以通过 score 函数定义:
$$
\begin{aligned}
\boldsymbol{I}_{f_{w}} &=E_{f_{w}}\left[\nabla_{w} \log f(w) \nabla_{w} \log f(w)^{\mathrm{T}}\right] \\
&=E_{f_{w}}\left[l_{f_{w}} l_{f_{w}}^{\mathrm{T}}\right]
\end{aligned}
$$


Fisher 信息矩阵有什么用处呢？我们这里有一个重要的结论：**一定条件下（概率分布函数要其备良好的性质）， Fisher 信息矩阵和KL 散度二阶导的相反数相等**。网上证明很多，这里我们就不再证明了。

通过前面的推演，我们的目标函数变为
$$
\begin{array}{l}
\operatorname{minimize}_{\Delta w} f(w)+\nabla_{w} f(w) \Delta w \\
\text { s.t. } \frac{1}{2} \Delta w^{\mathrm{T}} \boldsymbol{I}_{f_{w}} \Delta w<\epsilon
\end{array}
$$
这个有约束的问题可以通过拉格朗日乘子法表示为
$$
\operatorname{minimize}_{\Delta w} f(w)+\nabla_{w} f(w) \Delta w+\lambda\left[\frac{1}{2} \Delta w^{\mathrm{T}} I_{f_{w}} \Delta w-\epsilon\right]
$$
对公式进行求导，并求解对应的极限点，可以得到
$$
\begin{array}{l}
\nabla_{w} f(w)+\lambda \boldsymbol{I}_{f_{w}} \Delta w=0 \\
\Delta w=-\frac{1}{\lambda} \boldsymbol{I}_{f_{w}}^{-1} \nabla_{w} f(w)
\end{array}
$$
公式中的 $\frac{1}{\lambda}$ 可以当作梯度下降法的学习率类似的分量，那么自然梯度法的优化方 向就可以看作 $I_{f_{w}}^{-1} \nabla_{w} f(w),$ 与梯度下降法不同, 它需要额外求解 Fisher 信息矩阵的逆。

在这里我们可能会想到牛顿法，牛顿法是一个二阶梯度算法，它求解优化方向的公式为：
$$
g=-\nabla_{w}^{2} f(w)^{-1} \nabla_{w} f(w)
$$
我们很容易看出两个公式的区别与联系。根据 Fisher 信息矩阵的求解方法，自然梯度法可以变成一个一阶优化问题，也可以变成一个二阶优化问题。如果把它看作一个一阶优化问题，那么需要对优化步长做更多的考量; **如果把它看作一个二阶优化问题，那么牛顿法中可能遇到的一些问题同样会在自然梯度法中出现**。

那么到这里我们其实就可以将前面的策略单调提升的问题转化为下面的问题：
$$
\begin{array}{l}
\operatorname{minimize}_{\Delta \boldsymbol{w}} f(\boldsymbol{w})+\nabla_{\boldsymbol{w}} f(\boldsymbol{w}) \Delta \boldsymbol{w} \\
\text { s.t. } \quad \frac{1}{2} \Delta \boldsymbol{w}^{\mathrm{T}} \boldsymbol{I}_{f_{w}} \Delta \boldsymbol{w}<\epsilon
\end{array}
$$
其中 $w$ 表示参数， $f$ 表示待优化的函数, $I_{f_{w}}$ 表示 Fisher 信息矩阵。令策略 $\pi$ 的参数为 $\theta,$ 首先，对目标函数进行一阶泰勒展开，可以得到
$$
\begin{aligned}
L_{\pi_{\text {old }}}(\pi) &=L_{\pi_{\text {old }}}\left(\pi ; \theta_{\text {old }}+\Delta \theta\right) \\
& \simeq L_{\pi_{\text {old }}}\left(\pi_{\text {old }} ; \theta_{\text {old }}\right)+\left.\nabla_{\pi} L_{\pi_{\text {old }}}\left(\pi ; \theta_{\text {old }}\right)\right|_{\pi=\pi_{\text {old }}}(\Delta \theta)
\end{aligned}
$$
其次，对约束条件进行变换：
$$
\mathrm{KL}(f(\boldsymbol{w}) \| f(\boldsymbol{w}+\Delta \boldsymbol{w}))=-\frac{1}{2} \Delta \boldsymbol{w}^{\mathrm{T}} E_{f_{w}}\left[\nabla_{w}^{2} \log f(\boldsymbol{w})\right] \Delta \boldsymbol{w}
$$
可以对约束条件做第一步变换:
$$
\begin{aligned}
\bar{D}_{\mathrm{KL}}^{\rho_{\pi_{\text {old }}}}\left(\pi_{\text {old }}, \pi\right) &=\bar{D}_{\mathrm{KL}}^{\rho_{\pi_{\text {old }}}}\left(\pi\left(\theta_{\text {old }}\right), \pi\left(\theta_{\text {old }}+\Delta \theta\right)\right) \\
&=E_{s \sim \rho}\left[D_{\mathrm{KL}}\left(\pi\left(\theta_{\text {old }} \mid s\right) \| \pi\left(\theta_{\text {old }}+\Delta \theta \mid s\right)\right)\right] \\
&=E_{s \sim \rho}\left[-\frac{1}{2} \Delta \theta^{\mathrm{T}} E_{\pi_{\text {old }}}\left[\nabla_{\theta}^{2} \log \pi\left(\theta_{\text {old }} \mid s\right)\right] \Delta \theta\right]
\end{aligned}
$$
因为$: \boldsymbol{I}_{f_{w}}=-E_{f_{w}}\left[\nabla_{\boldsymbol{w}}^{2} \log f(\boldsymbol{w})\right],$ 继续推导得到
$$
=E_{\boldsymbol{s} \sim \rho}\left[\frac{1}{2} \Delta \theta^{\mathrm{T}} I_{\pi_{\mathrm{old}}}(s) \Delta \theta\right]
$$

$$
\simeq \frac{1}{N} \sum_{n=1}^{N}\left[\frac{1}{2} \Delta \theta^{\mathrm{T}} \boldsymbol{I}_{\pi_{\mathrm{old}}}(s) \Delta \theta\right]
$$
这样我们就从求解新策略的目标函数变成了求解策略参数更新量的目标函数:
$$
\begin{array}{l}
\left.\operatorname{maximize}_{\Delta \theta} \nabla_{\pi} L_{\pi_{\text {old }}}\left(\pi ; \theta_{\text {old }}\right)\right|_{\pi=\pi_{\text {old }}}(\Delta \theta) \\
\text { s.t. } \quad \frac{1}{N} \sum_{n=1}^{N}\left[\frac{1}{2} \Delta \theta^{\mathrm{T}} \boldsymbol{I}_{\pi_{\text {old }}}(s) \Delta \theta\right] \leqslant \epsilon
\end{array}
$$
虽然自然梯度法可以解决策略单调提升的问题，但是**它的计算复杂度过高，特别是矩阵求逆，它是立方级的复杂度**，对于大的网络而言，这种复杂度是无法接受的。

#### TRPO实现策略上的单调提升(Monotonic Improvement  with TRPO )

TRPO主要是针对我们前面提到的自然梯度法进行了两个方面的改进：1、用共轭梯度法简化求逆矩阵的高复杂度。 2、对KL divergence做的approximation中可能存在的约束违反的问题做了预防。

首先先来看第一个问题：

我们知道共轭梯度法可以用来求解 $A x=g$ 这样的问题，而参数更新量也可以表示成这样的形式:
$$
I_{\pi_{\text {old }}} \Delta \pi=\nabla_{\pi} L_{\pi_{\text {old }}}\left(\pi ; \theta_{\text {old }}\right)
$$
幸运的是，在共轭梯度法的计算过程中我们只需要矩阵 $A$ 参与运算，这样我们就不需要计算逆矩阵了。而且在共功梯度法的每一轮迭代中，它只参与了一次计算, 就是在计算的优化步长这一步:
$$
\alpha_{k}=\frac{r_{k}^{T} r_{k}}{p_{k}^{T} A p_{k}}
$$
虽然我们节省了一定的计算时间，但是我们依然要计算出矩阵 $A,$ 这个矩阵对深层神经网络这样复杂的模型来说同样是难以计算的，因此我们仍然需要找方法进行化简。幸运的是，矩阵 $A$ 同旁边的方向向量 $p$ 进行乘法会得到一个向量，这是简化算法的突破口。矩阵和向量乘积的结果的每一个元素都是由两个向量得到的，对应的计算公式为
$$
(A p)[i]=\sum_{j=1}^{N}\left(\frac{\partial}{\partial \pi_{i}} \frac{\partial}{\partial \pi_{j}} f(\pi)\right)[i, j] \times p[j]
$$
对于我们现在要解决的问题来说，求导和求和是可以互换的，所以得到
$$
=\frac{\partial}{\partial \pi_{i}}\left(\sum_{j=1}^{N} \frac{\partial}{\partial \pi_{j}} f(\pi)[j] \times p[j]\right)
$$
这样一来，我们先进行小括号内部的计算然后对结果进行求导，这样整体的复杂度将会大大降低。**原本的二阶求导变成了求两次一阶导数，运算量得到了大幅降低，运算速度得到了很大的提升**。解决了这个大问题，我们就可以使用共轭梯度法完成对优化方向的求解，令 $\nabla_{\pi} L_{\pi_{\text {old }}}\left(\pi ; \theta_{\text {old }}\right)$ 表 示残差，令 $\frac{1}{N} \sum_{n=1}^{N} I_{\pi_{\text {old }}}(s)$ 为共功梯度法的 $A,$ 就可以完成对优化方向 $s$ 的计算。

接下来我们来解决第二个问题：对KL divergence做的approximation中可能存在的约束违反的问题做了预防。这是这个问题可以通过确定更新的最大步长来决定：由于有约束条件的存在，优化的最大步长一定存在, 这样约束条件相当于为我们划定了一个置信区域 ( Trust Region )，以保证我们的优化满足策略提升的要求。前面我们利用共轭梯度法求出了方 向 $s,$ 现在令最大步长为 $\beta,$ 也就是说，参数更新的最大值为 $\beta s_{}$。 我们前面提到了问题的约束：
$$
\frac{1}{N} \sum_{n=1}^{N}\left[\frac{1}{2} \Delta \theta^{\mathrm{T}} \boldsymbol{I}_{\pi_{\mathrm{old}}} \Delta \theta\right] \leqslant \epsilon
$$
使用 $\beta s$ 代替 $\Delta \theta,$ 就可以得到
$$
\frac{1}{N} \sum_{n=1}^{N}\left[\frac{1}{2}(\beta s)^{\mathrm{T}} \boldsymbol{I}_{\pi_{o l d}}(s)(\beta \boldsymbol{s})\right]=\frac{1}{2} \beta^{2} \frac{1}{N} \sum_{n=1}^{N}\left[s^{\mathrm{T}} I_{\pi_{\mathrm{old}}}(s)\right] \leqslant \epsilon
$$
将公式进行整理，可以得到
$$
\beta \leqslant \sqrt{\frac{2 \epsilon}{\frac{1}{N} \sum_{n=1}^{N} s^{\mathrm{T}} \boldsymbol{I}_{\pi_{\mathrm{old}}}}}
$$
这样我们就知道了满足约束条件下的最大步长。与前面的方法类似，这里同样可 以将 Fisher 信息矩阵和右边的方向向量相乘，这样可以用两个一阶导计算代替本的 二阶导计算，方法和前面介绍的类似。
最后，我们使用 backtrack 的线搜索方法找到满足优化条件的合适步长，也就是找 到一个 $\pi(\theta)=\pi\left(\theta_{\text {old }}+\Delta \theta\right),$ 使得它比 $\pi_{\text {old }}$ 产生足够的提升。具体方法为 : 先尝试以 $\beta$ 为步长的情况下，策略提升是否可以满足，如果已经满足则步长选择结束; 如果无法满 足，则将步长减少一半，再进行测试，直到满足为止。如下图所示：

![img](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/v2-8690c9231a1994106d78b5b761fd4029_720w.jpg)

最后我们附上TRPO的伪码：

![\begin{algorithm}[H]     \caption{Trust Region Policy Optimization}     \label{alg1} \begin{algorithmic}[1]     \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$     \STATE Hyperparameters: KL-divergence limit $\delta$, backtracking coefficient $\alpha$, maximum number of backtracking steps $K$     \FOR{$k = 0,1,2,...$}     \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.     \STATE Compute rewards-to-go $\hat{R}_t$.     \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.     \STATE Estimate policy gradient as         \begin{equation*}         \hat{g}_k = \frac{1}{|{\mathcal D}_k|} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \left. \nabla_{\theta} \log\pi_{\theta}(a_t|s_t)\right|_{\theta_k} \hat{A}_t.         \end{equation*}     \STATE Use the conjugate gradient algorithm to compute         \begin{equation*}         \hat{x}_k \approx \hat{H}_k^{-1} \hat{g}_k,         \end{equation*}         where $\hat{H}_k$ is the Hessian of the sample average KL-divergence.     \STATE Update the policy by backtracking line search with         \begin{equation*}         \theta_{k+1} = \theta_k + \alpha^j \sqrt{ \frac{2\delta}{\hat{x}_k^T \hat{H}_k \hat{x}_k}} \hat{x}_k,         \end{equation*}         where $j \in \{0, 1, 2, ... K\}$ is the smallest value which improves the sample loss and satisfies the sample KL-divergence constraint.     \STATE Fit value function by regression on mean-squared error:         \begin{equation*}         \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,         \end{equation*}         typically via some gradient descent algorithm.     \ENDFOR \end{algorithmic} \end{algorithm}](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/5808864ea60ebc3702704717d9f4c3773c90540d.svg)

#### PPO实现策略上的单调提升(Monotonic Improvement  with PPO )

相比上面使用natural gradients进行KL constraint的近似，PPO则另辟蹊径，它并不直接求解这个约束，而是通过一些启发式的方法达到这个目的，论文中主要提出PPO1和PPO2两类算法：

**PPO1：Adaptive KL penalty**，它直接将KL divergence从constraint转化为penalize项，作为目标函数的一部分，每次更新时候则计算当前迭代对KL divergence的违反程度，如果KL值过大，则增大它的系数，强化这个penalty term的重要程度，反之亦然。
$$
L_{t}^{\mathrm{KLPEN}}(\theta)=\frac{1}{N} \sum_{t=1}^{N}\left[\frac{\pi(a \mid s)}{\pi_{\mathrm{old}}(\boldsymbol{a} \mid s)} A_{\pi_{\mathrm{old}}}(\boldsymbol{s}, \boldsymbol{a})-\beta \mathrm{KL}\left[\pi_{\theta_{\mathrm{old}}}\left(\cdot \mid s_{t}\right), \pi_{\theta}\left(\cdot \mid s_{t}\right)\right]\right]
$$
我们会预先设定超参数提供我们能接受的策略之间差异的最大值和最小值，如果实际策略的差异比最大值要大，那么我们就会增大$\beta$的值，相当于增大KL约束限制；如果如果实际策略的差异比最小值要小，那么我们就会减小$\beta$的值，相当于减小KL约束限制。

![image-20210326113036390](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210326113036390.png)

**PPO2：Clipped Surrogate Objective**，对目标做clip。前面使用到KL divergence是要约束policy space，从而约束trajectory的distribution。那么这里则直接对这个概率比值做clip，设置一个阈值，直接从根源上限制policy space的差异。
$$
L^{C P I}(\theta)=\hat{\mathbb{E}}_{t}\left[\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)} \hat{A}_{t}\right]=\hat{\mathbb{E}}_{t}\left[r_{t}(\theta) \hat{A}_{t}\right]
$$

$$
L^{C L I P}(\theta)=\hat{\mathbb{E}}_{t}\left[\min \left(r_{t}(\theta) \hat{A}_{t}, \operatorname{clip}\left(r_{t}(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right)\right]
$$

这里稍微解释一下clip操作，clip指的是，如果第一项小于第二项，那么输出的结果就是第二项，如果第一项大于第三项，那么输出的结果就是第三项。可以看出目标函数为了确保算法能达到 TRPO 中置信区域的效果, 为算法上了两把 “锁”: 第一把锁是对新旧策略概率比率 $r_{t}(\theta)$ 的限制, 这个比率只能限制在 $[1-\epsilon, 1+\epsilon],$ 这确保了每一次的更新不会有太大的波动; 第二把锁就是 min 函数，它将在两个结果 中选择一个较低值作为结果。如果我们能够将较低值优化到令人满意的程度，那么对于其他的情况，模型一定会表现得更好。

这里想引用一下李宏毅老师的课上讲PPO的那张图来解释一下Clip操作，首先我们看一下PPO2里面的整合的目标函数：
$$
\begin{aligned}
J_{P P O 2}^{\theta^{k}}(\theta) \approx \sum_{\left(s_{t}, a_{t}\right)} \min (& \frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right) \\
&\left.\operatorname{clip}\left(\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^{k}}\left(s_{t}, a_{t}\right)\right)
\end{aligned}
$$
那么我们来画一个图，这个横轴表示的意思是策略之间从比值，纵轴就是最终的输出，那么其实下图里面的这挑浅蓝色的虚线就是clip函数

![image-20210326124519262](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210326124519262.png)

而$$
\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right)
$$就是绿色的虚线。

![image-20210326124624751](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210326124624751.png)

那如果当A>0,也就是表示我们现在的这个（s，a）是好的时候，我们会希望增大$$
\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right)
$$，来提高这种好的（s，a）出现的概率，但是，我们不希望它无限制的增大，因为当$$
\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right)
$$很大的时候，采样策略和实际的策略之间就会差很多，那么增加$$
\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right)
$$所带来的坏处就会大于好处，所以我们使用clip操作让最终的输出是红色的这条线：

![image-20210326125129298](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210326125129298.png)

那如果当A<0,也就是表示我们现在的这个（s，a）是不好的时候，我们会希望减小$$
\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right)
$$，来减小这种不好的（s，a）出现的概率，但是，我们不希望它无限制地变小，因为当$$
\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right)
$$很小的时候，采样策略和实际的策略之间就会差很多，那么减小$$
\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right)
$$所带来的坏处就会大于好处，所以我们使用clip操作让最终的输出是红色的这条线：

![image-20210326125204541](https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20210326125204541.png)

这也就是clip函数所带来的好处！

#### GAE

GAE全称是generalized advantage estimator，几乎所有最先进的policy gradient算法实现里面都使用了该技术。

我们已经知道在策略梯度法中，如果直接使用 On-Policy 的方法交互采样，并用 每一时刻的回报作为梯度中的长期回报估计 $\sum_{t^{\prime}=t}^{T} \gamma^{t^{\prime}-t} \boldsymbol{r}_{t^{\prime}},$ 会使算法产生较大的波动，换句话说，梯度的方差会比较大。如果采用 Actor-Critic 的方法，通过模型估计状态的价值，那么模型优化的方差会减小，但是由于函数拟合的问题，这个方法会产生一定偏差。因此问题的关键就在于如何平衡偏差和方差带来的影响。
Actor Critic 的价值梯度可以表示为
$$
\nabla_{\theta} J(\theta)=E_{\boldsymbol{s}, a \sim \boldsymbol{\tau}}\left[\sum_{t=0}^{\infty} A^{\pi, \gamma}\left(\boldsymbol{s}_{t}, \boldsymbol{a}_{t}\right) \nabla_{\theta} \log \pi_{\theta}\left(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t}\right)\right]
$$
其中
$$
\begin{aligned}
A^{\pi, \gamma}\left(s_{t}, a_{t}\right) &=Q^{\pi, \gamma}\left(s_{t}, a_{t}\right)-V^{\pi, \gamma}\left(s_{t}\right) \\
Q^{\pi, \gamma}\left(s_{t}, a_{t}\right) &=E_{s_{t+1}, a_{t+1} \sim \tau}\left[\sum_{l=0}^{\infty} \gamma^{l} r_{t+l}\right] \\
V^{\pi, \gamma}\left(s_{t}\right) &=E_{s_{t+1}, a_{t} \sim \tau}\left[\sum_{l=0}^{\infty} \gamma^{l} r_{t+l}\right]
\end{aligned}
$$
总的来说， $A^{\pi, \gamma}\left(s_{t}, a_{t}\right)$ 已经可以做到在保持无偏差的情况下，尽可能地降低方差值。如果我们能通过学习得到一个完美的优势函数，模型就可以得到很好的表现。但实际中直接学习优势函数比较困难，我们往往需要组合其他函数得到优势函数，同时还需要考虑偏差和方差对模型的影响，为此我们给出了一个定义: $\gamma$ -just。当一个函数 $\hat{A}_{t}$ 满足 $\gamma$ -just 条件时，它就满足下面的公式:
                                                      $E_{s_{0}, a_{0}, \ldots \sim \tau}\left[\hat{A}_{t}\left(s_{0: \infty}, a_{0: \infty}\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)\right]=E_{s_{0}, a_{0}, \ldots \sim \tau}\left[A^{\pi, \gamma}\left(s_{t}, a_{t}\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)\right]$
如果我们找到的估计函数能够满足上面的公式，它就可以用来替换优势函数。经过推导分析我们发现， $r_{t}+\gamma V^{\pi, \gamma}\left(s_{t+1}\right)-V^{\pi, \gamma}\left(s_{t}\right)$ 满足上述条件，是一个合格的替换 项，于是后面的工作将围绕它替换进行。

令 $V$ 为一个近似的值函数，我们定义 $\delta_{t}^{V}=r_{t}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right),$ 这里的 $\delta_{t}^{V}$ 可 以作为 $a_{t}$ 的一个优势价值估计。如果上面的公式中 $V=V^{\pi, \gamma},$ 那么 $\delta_{t}^{V}$ 就是一个 $\gamma$ -just 的估计函数，它可以得到 $A^{\pi, \gamma}$ 的一个无偏估计:
$$
\begin{aligned}
E_{\boldsymbol{s}_{t+1}}\left[\boldsymbol{\delta}_{t}^{V^{\pi, \gamma}}\right] &=E_{\boldsymbol{s}_{t+1}}\left[\boldsymbol{r}_{t}+\gamma V^{\pi, \gamma}\left(s_{t+1}\right)-V^{\pi, \gamma}\left(s_{t}\right)\right] \\
&=E_{s_{t+1}}\left[Q^{\pi, \gamma}\left(s_{t}, \boldsymbol{a}_{t}\right)-V^{\pi, \gamma}\left(s_{t}\right)\right]=A^{\pi, \gamma}\left(\boldsymbol{s}_{t}, \boldsymbol{a}_{t}\right)
\end{aligned}
$$
接下来我们考虑 $n$ 步的优势函数估计，并用 $\hat{A}_{t}^{(k)}$ 表示，可以得到
$$
\begin{array}{l}
\hat{A}_{t}^{(1)}=\delta_{t}^{V}=-V\left(s_{t}\right)+r_{t}+\gamma V\left(s_{t+1}\right) \\
\hat{A}_{t}^{(2)}=\delta_{t}^{V}+\gamma \delta_{t+1}^{V} \\
=-V\left(s_{t}\right)+r_{t}+\gamma V\left(s_{t+1}\right)+\gamma\left(-V\left(s_{t+1}\right)+r_{t+1}+\gamma V\left(s_{t+2}\right)\right) \\
=-V\left(s_{t}\right)+r_{t}+\gamma r_{t+1}+\gamma^{2} V\left(s_{t+2}\right)
\end{array}
$$
依此类推，可以得到
$$
\hat{A}_{t}^{(\infty)}=\sum_{l=0}^{\infty} \gamma^{l} \delta_{t+l}^{V}=-V\left(s_{t}\right)+r_{t}+\gamma \boldsymbol{r}_{t+1}+\cdots+\gamma^{k} \boldsymbol{r}_{t+k}+\cdots
$$
我们知道 $\gamma$ 是一个小于 1 的数，随着 $k$ 趋近于无穷大，最终 $\gamma^{\infty} V\left(s_{t+\infty}\right) \rightarrow 0$, 所 以 $\hat{A}_{t}^{(\infty)}$ 这一项相当于用蒙特卡罗法对优势函数进行估计。此时我们看到，随着估计步数的增加，估计值的偏差逐渐变小，方差逐渐变大。如果我们能将这些估计值同时考虑在内，是就可以在偏差和方差之间找到更好的平衡。
$$
\begin{aligned}
\hat{A}_{t}^{\mathrm{GAE}(\gamma, \lambda)} &=(1-\lambda)\left(\hat{A}_{t}^{(1)}+\lambda \hat{A}_{t}^{(2)}+\lambda^{2} \hat{A}_{t}^{(3)}+\cdots\right) \\
&=(1-\lambda)\left(\delta_{t}^{V}+\lambda\left(\delta_{t}^{V}+\delta_{t+1}^{V}\right)+\lambda^{2}\left(\delta_{t}^{V}+\gamma \delta_{t+1}^{V}+\gamma^{2} \delta_{t+2}^{V}\right)+\cdots\right) \\
&=(1-\lambda)\left(\delta_{t}^{V}\left(1+\lambda+\lambda^{2}+\cdots\right)+\gamma \delta_{t+1}^{V}\left(\lambda+\lambda^{2}+\lambda^{3}+\cdots\right)\right.\\
&\left.+\gamma^{2} \delta_{t+2}^{V}\left(\lambda^{2}+\lambda^{3}+\lambda^{4}+\cdots\right)+\cdots\right) \\
&=(1-\lambda)\left(\delta_{t}^{V}\left(\frac{1}{1-\lambda}\right)+\gamma \delta_{t+1}^{V}\left(\frac{\lambda}{1-\lambda}\right)+\gamma^{2} \delta_{t+2}^{V}\left(\frac{\lambda^{2}}{1-\lambda}\right)+\cdots\right) \\
&=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}^{V}
\end{aligned}
$$
我们发现这个公式的最终形式比较简洁，虽然我们引人了一个新的超参数，但是公式并没有复杂太多。此时我们的估计值在偏差和方差方面得到了更好的平衡，我们可以分别计算 $\lambda$ 等于 0 和 1 时的值
$$
\begin{array}{lrl}
\operatorname{GAE}(\gamma, 0): & \hat{A}_{t}:=\delta_{t} & =r_{t}+\gamma v\left(s_{t+1}\right)-v\left(s_{t}\right) \\
\operatorname{GAE}(\gamma, 1): & \hat{A}_{t}:=\sum_{l=0}^{\infty} \gamma^{l} \delta_{t+l} & =\operatorname{sum}_{t=0}^{\infty} \gamma^{l} r_{t+l}-v\left(s_{t}\right)
\end{array}
$$
可以看出，当 $\lambda=0$ 时，算法等同于计算 TD-Error，这是一个方差较低但偏差较高的算法; 

当 $\lambda=1$ 时，算法变成蒙特卡罗目标值和价值估计的差，这是一个偏差较低但方差较高的算法。我们可以通过调整 $\lambda$ 值使模型在两者之间得到更好的平衡。因此,
我们可以用它代替前面公式中的优势函数，此时计算的公式就变为
$$
\nabla_{\theta} J(\theta)=E_{s, a \sim \tau}\left[\sum_{t=0}^{\infty} \hat{A}_{t}^{\mathrm{GAE}(\gamma, \lambda)} \nabla_{\theta} \log \pi_{\theta}\left(\boldsymbol{a}_{t} \mid s_{t}\right)\right]
$$
当时第一次看GAE的时候很不理解为什么GAE选定了 $\delta_{t}^{V}=r_{t}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)$这种形式，后面看到吴恩达老师1998年[reward shaping的论文](http://luthuli.cs.uiuc.edu/~daf/courses/games/AIpapers/ml99-shaping.pdf)，在做reward shaping的时候将reward加上一个势能函数的差值就保障了加入reward shaping之后策略没有变坏：
$$
\tilde{r}\left(s, a, s^{\prime}\right)=r\left(s, a, s^{\prime}\right)+\gamma \Phi\left(s^{\prime}\right)-\Phi(s)
$$
感觉GAE就是将V函数替代r函数的一种操作，也就是说GAE是一种更加“长视”的reward shaping的方法？不知道这么理解对不对，欢迎大家讨论。

#### 基于GAE和TRPO的值函数优化

在介绍 论文 High-Dimensional Continuous Control Using Generalized Advantage Estimation 中，算法除了将 TRPO 方法应用在策略的训练上，还将 TRPO 应用到了价值网络的训练上。价值网络的目标函数为
$$
\operatorname{minimize}_{\phi} \sum_{n=1}^{N}\left\|V_{\phi}\left(s_{n}\right)-\hat{V}_{n}\right\|^{2}
$$
我们可以使用类似 Trust Region 的方法对目标值函数进行限定。限定的核心思想是希望新的价值模型与旧模型不要相差过大，那么我们首先可以得到原始值函数和 $\mathrm{GAE}$方法的估计值的均方误差 $\sigma^{2}$ :
$$
\sigma^{2}=\frac{1}{N} \sum_{n=1}^{N}\left\|V_{\phi_{\text {old }}}\left(s_{n}\right)-\hat{V}_{n}\right\|^{2}
$$
我们希望新旧模型的价值估计差不要过大，为了使约束具有一定的自适应性，我们将限定值设定为和 $\sigma^{2}$ 相关的一个值，于是我们的问题变为
$$
\begin{array}{l}
\operatorname{minimize}_{\phi} \sum_{n=1}^{N}\left\|V_{\phi}\left(s_{n}\right)-\hat{V}_{n}\right\|^{2} \\
\text { s.t. } \quad \frac{1}{N} \sum_{n=1}^{N} \frac{\left\|V_{\phi}\left(s_{n}\right)-V_{\phi_{\mathrm{old}}}\left(s_{n}\right)\right\|^{2}}{2 \sigma^{2}} \leqslant \epsilon
\end{array}
$$
这个问题的描述和 TRPO 中对策略网络的描述很类似，约束条件也和 TRPO 中的$\mathrm{KL}$ 散度约束类似，这样我们同样可以把问题转变为一阶目标函数和二阶约束条件的形式:
$$
\begin{array}{l}
\operatorname{minimize}_{\phi} \nabla_{\phi} V_{\phi}\left(s_{n}\right)^{\mathrm{T}}\left(\phi-\phi_{\text {old }}\right) \\
\text { s.t. } \quad \frac{1}{N} \sum_{n=1}^{N}\left(\phi-\phi_{\text {old }}\right)^{\mathrm{T}} H\left(\phi-\phi_{\text {old }}\right) \leqslant \epsilon
\end{array}
$$
其中 $H=\frac{1}{N} \sum_{n} j_{n} j_{n}^{\mathrm{T}}, j_{n}=\nabla_{\phi} V_{\phi}\left(s_{n}\right)_{\circ}$ 得到了这样的形式，我们就可以用 Fisher 信息矩阵近似值函数的二阶导，同时利用共轭梯度法进行优化，通过计算矩阵和向量的乘积来减少计算。由于这部分的计算过程和 TRPO 类似，这里就不再赘述。通过这种估计方法，我们在使用 TRPO方法估计时可以获得更稳定的模型，效果也比原始模型更好。